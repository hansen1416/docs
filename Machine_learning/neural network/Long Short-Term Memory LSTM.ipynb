{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM based neural networks are a variant of RNNs designed to handle long-range dependencies in the input sequence more accurately than vanilla RNNs. \n",
    "\n",
    "- LSTMs have a similar chain-like structure to RNNs; however, they comprise a four-layer neural network instead of a single layer network for RNNs. An LSTM is composed of 4 components: a cell, an input gate, an output gate and a forget gate. These allow RNNs to remember or forget words over arbitrary time intervals by regulating the flow of information in and out of the cell.\n",
    "\n",
    "- However, there is a limitation to how much information can be saved as there is still a complex sequential path from previous cells to the current cell. This limits the length of sequences that an LSTM could remember to just a few hundred words. An additional pitfall is that LSTMs are very difficult to train due to high computational requirements. Due to their sequential nature, they are hard to parallelize, limiting their ability to take advantage of modern computing devices such as GPUs and TPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

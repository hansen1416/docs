{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear Classifier\n",
    "\n",
    "### Loss function im classification\n",
    "\n",
    "Loss function - error rate\n",
    "\n",
    "$L(a,x) = \\frac{1}{N} \\sum^N_{i=1}[a(x_i) \\ne y_i]$\n",
    "\n",
    "sometimes accuracy is measured:\n",
    "\n",
    "$L(a,x) = \\frac{1}{N} \\sum^N_{i=1}[a(x_i) = y_i]$\n",
    "\n",
    "Indicator function\n",
    "\n",
    "$[A]=\\begin{cases} 1, \\text{if A is True} \\\\ 0, \\text{if A is False} \\end{cases}$\n",
    "\n",
    "---\n",
    "\n",
    "### Margin\n",
    "\n",
    "Loss function\n",
    "\n",
    "$L(w, X) = \\frac{1}{N} \\sum^{N}_{i=1}[sign(\\langle w, x_i \\rangle) \\ne y_i]$\n",
    "\n",
    "Alternative formulation \n",
    "\n",
    "$L(w, X) = \\frac{1}{N} \\sum^{N}_{i=1}[y_i\\langle w, x_i \\rangle < 0]$\n",
    "\n",
    "Indicator - non-differentiable function (can't use gradient decent to train)\n",
    "\n",
    "_for all values of margin that are smaller than 0, it is equal to 1, and for all values larger than 0, it is equal to 0._\n",
    "\n",
    "### upper bound\n",
    "\n",
    "- To train it, we found a function which __upper bound__ our error rate. And use it instead of the error rate loss to train our classifier.\n",
    "\n",
    "- We minimize our upper bound\n",
    "\n",
    "- Hopefully, it will automatically reduce the error rate\n",
    "\n",
    "_examples_\n",
    "\n",
    "- logistic, $\\tilde{l}(M) = log(1 + e^{-M})$\n",
    "- hinge loss, $\\tilde{l}(M) = max(0, 1-M)$\n",
    "- exponential, $\\tilde{l}(M) = e^{-M}$\n",
    "- sigmoid, $\\tilde{l}(M) = \\frac{2}{1+e^M}$\n",
    "\n",
    "Assume we choose logstic function.\n",
    "\n",
    "We can apply gradient descent to optimize our loss.\n",
    "\n",
    "$\\tilde{L}(w, X) = \\frac{1}{N} \\sum^{N}_{i=1} log(1+exp(-y_i\\langle w,x_i \\rangle)) -> min_w$\n",
    "\n",
    "We can add regularization, just as we did with linear regression\n",
    "\n",
    "$min_w \\tilde{L}(w,X) + \\lambda \\|w\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Metrics in Classification\n",
    "\n",
    "Accuracy could be bad for imbalanced data. \n",
    "\n",
    "__Confusion matirix__\n",
    "\n",
    "|  | y=1 | y=-1 |\n",
    "|--|--|--|\n",
    "| a(x)=1 | TP | FP |\n",
    "| a(x)=-1 | FN | TN |\n",
    "\n",
    "__Precision:__\n",
    "\n",
    "Can we trust a classifier, when it attributes an object to a positive class?\n",
    "\n",
    "$precision(a, X) = \\frac{TP}{TP +FP}$\n",
    "\n",
    "__Recall__\n",
    "\n",
    "Which porpoortion of objects from the positive class was the model able to detect?\n",
    "\n",
    "$recall(a, X) = \\frac{TP}{TP + FN}$\n",
    "\n",
    "__$F_1$ score (harmonic mean)__\n",
    "\n",
    "$\\large F = \\frac{2 \\times precision \\times recall}{precision + recall}$\n",
    "\n",
    "__$F_{\\beta}$ score__\n",
    "\n",
    "$\\large F = \\frac{(1+\\beta^2)\\times precision \\times recall}{\\beta^2 \\times precision + recall}$\n",
    "\n",
    "$\\beta = 0.5-$ precision that is more important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

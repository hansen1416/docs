{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Binary classification task: $\\mathbb{Y} = \\{-1, +1\\}$\n",
    "\n",
    "Linear classifier\n",
    "\n",
    "$a(x)=sign(b(x) - t) = sign(\\langle w, x\\rangle - t)$\n",
    "\n",
    "Error rate loss function\n",
    "\n",
    "$min_w \\frac{1}{N} \\sum^{N}_{i=1} [y_i \\langle w, x_i \\rangle < 0] = min_w \\frac{1}{N} \\sum^{N}_{i=1} [M_i < 0]$\n",
    "\n",
    "We can optimize differetiable upper bound\n",
    "\n",
    "$min_w \\frac{1}{N} \\sum^{N}_{i=1} \\log(1+\\exp(-M_i))$\n",
    "\n",
    "__Can we use $b(x)=\\langle w, x \\rangle$ as a probability estimate?__\n",
    "\n",
    "__Linear classifier__\n",
    "\n",
    "Let us convert the output to [0, 1]\n",
    "\n",
    "Sigmoid function $\\sigma(\\langle w, x \\rangle) = \\large \\frac{1}{1+\\exp(-\\langle w, x \\rangle)}$\n",
    "\n",
    "__Logistic Regression__\n",
    "\n",
    "Binary classification task: $\\mathbb{Y} \\in {-1, +1}$\n",
    "\n",
    "Predicted probability:  \n",
    "\n",
    "$P(y_i=1) = b(x_i)$\n",
    "\n",
    "Use sigmoid function to map outputs to the range from 0 to 1\n",
    "\n",
    "$b(x)= \\sigma(\\langle w, x \\rangle) = \\large \\frac{1}{1+\\exp(-\\langle w, x \\rangle)}$\n",
    "\n",
    "- In some tasks it is important to predict class probabilities\n",
    "\n",
    "- We can apply sigmoid function to the output of the model to get numbers between 0 and 1\n",
    "\n",
    "- Finally we want to train our model in such a way, that they are interpreted as probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE for Logistic Regression\n",
    "\n",
    "$y_i \\in \\{-1, +1\\}$\n",
    "\n",
    "$P(y_i=1|x_i) = \\large \\frac{1}{1+\\exp(-\\langle w, x \\rangle)}$\n",
    "\n",
    "$P(y_i=-1|x_i) = 1-P(y_i=1|x_i) = \\large \\frac{1}{1+\\exp(\\langle w, x \\rangle)}$\n",
    "\n",
    "use maximum likelihood estimation in order to find optimal parameters $w$.\n",
    "\n",
    "MLE: It takes the likelihood of a model and maximize it with respect to the desired parameters.\n",
    "\n",
    "$\\log L = \\log \\Pi^N_{i=1} P(y_i|x_i) \\to max_w$\n",
    "\n",
    "$ - \\log \\Pi^N_{i=1} P(y_i|x_i) \\to min_w$\n",
    "\n",
    "---\n",
    "\n",
    "$- \\sum^N_{i=1} \\log P(y_i|x_i) $ (from logoritm of probobility product, to sum of logoritm of probobility)\n",
    "\n",
    "$= -\\sum^N_{i=1} [[y_i=1] \\log \\frac{1}{1+\\exp(-\\langle w, x_i \\rangle)} + [y_i=-1] \\log \\frac{1}{1+\\exp(\\langle w, x_i \\rangle)}]$ (we have two options, $y\\in \\{-1, +1\\}$)\n",
    "\n",
    "$=\\sum^N_{i=1} [[y_i=1] \\log (1+\\exp(-\\langle w, x_i \\rangle)) + [y_i=-1] \\log (1+\\exp(\\langle w, x_i \\rangle))]$\n",
    "\n",
    "$= \\sum^N_{i=1} \\log(1 + \\exp(-y_i\\langle w, x_i \\rangle)) \\to min_w$ \n",
    "\n",
    "this is the final loss we will minimizing with respect to $w$\n",
    "\n",
    "where $y_i\\langle w, x_i \\rangle$ is __margin__ and we saw that if it is positive, then we say that the sign of the scalar product and the target variable are the same, meaning that our classifier is correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

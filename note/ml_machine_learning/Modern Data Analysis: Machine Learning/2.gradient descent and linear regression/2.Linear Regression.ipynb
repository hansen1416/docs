{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate linear regression\n",
    "\n",
    "$X \\in R^{N \\times d}, y \\in R^{N}, w \\in R^{d}$\n",
    "\n",
    "Model: $ a(x) = X w$\n",
    "\n",
    "MSE: $ L(w) = \\frac{1}{N}\\|Xw - y\\|^2 $\n",
    "\n",
    "Goal: $w^*: min_w L(w)$\n",
    "\n",
    "Solve:\n",
    "\n",
    "$\\nabla_w L(w) = 0$\n",
    "\n",
    "$\\nabla_w\\| Xw-y \\|^2 = \\frac{1}{N}X^T 2(Xw-y)=0$\n",
    "\n",
    "$X^T (Xw^*-y)=0$\n",
    "\n",
    "$X^TXw^*-X^Ty=0$\n",
    "\n",
    "$X^TXw^* = X^Ty$\n",
    "\n",
    "$(X^TX)^{-1}X^TX w^* = X^Ty(X^TX)^{-1}$\n",
    "\n",
    "$w^* = (X^TX)^{-1} X^Ty$\n",
    "\n",
    "\n",
    "---\n",
    "Euclidean norm:\n",
    "\n",
    "1. $a \\in R^d\\;\\;\\;\\; \\|a\\|^2 = \\sum^d_{i=1} a^2_i$\n",
    "\n",
    "Gradient of such function $\\nabla_a \\| a\\|^2 = \\begin{pmatrix}\n",
    "\\frac{\\partial \\| a\\|^2}{\\partial a_1} \\\\\n",
    "\\frac{\\partial \\| a\\|^2}{\\partial a_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "2 a_1 \\\\\n",
    "2 a_2 \\\\\n",
    "\\vdots \\\\\n",
    "2 a_d \\\\\n",
    "\\end{pmatrix}=2a\n",
    "\\;\\;\\;\\;\\;$($a_i$ exists in only one term of $\\sum^d_{i=1} a^2_i$)\n",
    "\n",
    "---\n",
    "\n",
    "2 $\\nabla_w(Xw) = X^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression\n",
    "\n",
    "$a(x) = <w, x>$\n",
    "\n",
    "$L(w) = \\frac{1}{N} \\| Xw-y \\|^2, X \\in R^{n\\times k}, y = R^{n}$\n",
    "\n",
    "$w^* = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "It's not always easy or even not always possible to use this formula to find optimal parameters, becasue inversion of metrics is not a trivial task.\n",
    "\n",
    "$X^TX \\in R^{k\\times k}$\n",
    "\n",
    "1. complexity $O(k^3)$\n",
    "\n",
    "2. Stability. Sometimes matrices do not have inverse at all. It happens for example, when we have linear dependent lines or columns in our matrix. In a more like practical case that happens if your features are linearly dependent. Or even if you don't have linear dependent features, where you can obtain one feature from another or just by taking linear combination of other features. But your features are highly correlated, then computing the inverse may be very unstable operation. Because we're using approximate algorithms to do that.\n",
    "\n",
    "- $w^0 \\sim N(O, I_{k\\times k})$\n",
    "\n",
    "For t in 1 ... Max_iter:\n",
    "\n",
    "$w^t = w^{t-1} - \\eta \\nabla_w L(w^{t-1})$\n",
    "\n",
    "if $\\| w^t - w^{t-1} \\|_2 < \\epsilon$:\n",
    "    break\n",
    "    \n",
    "$\\nabla_w L(w) = \\frac{2}{N} X^T(Xw-y) = \\frac{2}{N}(X^TXw-X^Ty)$\n",
    "\n",
    "$w^t = w^{t-1} - \\eta \\frac{2}{N}(X^TX w^{t-1} - X^Ty)$\n",
    "\n",
    "Calculate $X^TX$ and $X^Ty$ at the begining, and use them through out the entire process.\n",
    "\n",
    "Complex part is only $X^TX w^{t-1}$. much better than calculate inverse matrix. it's good for model has correlated features or a lot of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses in Linear Regression\n",
    "\n",
    "## Mean squared error\n",
    "\n",
    "one of the most popular loss functions, performed bad when there is outlier\n",
    "\n",
    "$L(a, X) = \\frac{1}{N} \\sum^{N}_{n=1}(a(x_n) - y_n)^2$\n",
    "\n",
    "## Mean absolute error\n",
    "\n",
    "using absolute values instead of square, performed better when there is outlier\n",
    "\n",
    "$L(a, X) = \\frac{1}{N} \\sum^{N}_{n=1} |a(x_n)-y_n|$\n",
    "\n",
    "## Huber loss\n",
    "\n",
    "use MSE around center, MAE on the tail\n",
    "\n",
    "$l_H(y,a)= \\begin{cases}\n",
    "\\frac{1}{2}(y-a)^2, |y-a| < \\delta \\\\\n",
    "\\delta (|y-a|-\\frac{1}{2}\\delta), |y-a| \\ge \\delta \\\\\n",
    "\\end{cases}$\n",
    "\n",
    "$L(a,X) = \\frac{1}{N}\\sum^{N}_{n=1}l_H(y_n, a(x_n))$\n",
    "\n",
    "Above error functions are relative.\n",
    "\n",
    "---\n",
    "\n",
    "## Mean absolute percentage error\n",
    "\n",
    "Loss similar to MAE, measure in percentage.\n",
    "\n",
    "$L(a,X)=\\frac{100\\%}{N}\\sum^{N}_{n=1}|\\frac{a(x_n)-y_n}{y_n}|$\n",
    "\n",
    "- Non-symmetric function\n",
    "\n",
    "- Give preference to negative error\n",
    "\n",
    "## Symmetric MAPE\n",
    "\n",
    "Add some symmetry\n",
    "\n",
    "$L(a,X) = \\frac{100\\%}{N}\\sum^N_{n=1}\\frac{|y_n-a(x_n)|}{(|y_n|+|a(x_n)|)/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation and Feature Importance\n",
    "\n",
    "- It is worth comparing the weights only if features are scaled.\n",
    "\n",
    "- Estimate importance of a feature by removing it from the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

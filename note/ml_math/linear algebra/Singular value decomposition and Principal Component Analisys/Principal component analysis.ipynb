{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The idea of dimensionality reduction__\n",
    "\n",
    "Suppose that $X$ is $m\\times n$, where the columns $X^i$ of $X$ represent variables,\n",
    "\n",
    "The rows $X_i$ of $X$ represent the sample elements.\n",
    "\n",
    "The aim of the dimensionality reduction: to introduce new variables such that each $X_i$ is (approximately) represented via __smaller number of parameters__.\n",
    "\n",
    "To do this, we want to introduce __new co-ordinates__ in $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First step: centering the data__\n",
    "\n",
    "Let $\\overline{X} = (X_1 + \\dotsm + X_m)/m = (1/m)(1,\\dots,1)X$ be the mean row of $X$. Then $X$ be the origin of our new coordinate system.\n",
    "\n",
    "Let $A$ be the matrix such that each row $A_i$ is $X_i - \\overline{X}$. If $\\widetilde{X}$ is the matrix of the same size as $X$ where each row is replaced by $\\overline{X}$, then $A=X-\\widetilde{X}=X-(1/m)(1,\\dots,1)^T(1,\\dots,1)X$.\n",
    "\n",
    "Now the mean row of $A$ is 0.\n",
    "\n",
    "(Sometimes people also standartize the data, but this is not necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dimensionality reduction and the basis change__\n",
    "\n",
    "Supopse we want to reduce the dimensionality of the problem (that is, the number of variables) from $n$ to $r$.\n",
    "\n",
    "We want to find an r-dimensional subspace $L$ of $\\mathbb{R}^n$ and project our data onto $L$. $A$ basis of $L$ will give new co-ordinate axes, that is, new variables.\n",
    "\n",
    "Problem: how to find L and the new axes to minimize the information lost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Second step SVD and principal axes__\n",
    "\n",
    "consider the SVD $A=U\\Sigma V^T$.\n",
    "\n",
    "Then the columns of $V$ are called _princial axes_.\n",
    "\n",
    "The first $r$ columns $V^1, \\dots , V^r$ form an orthonormal basis of $L$.  \n",
    "_$V^r$ in descending order. Roughly speaking, the eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data_\n",
    "\n",
    "Let $x \\in \\mathbb{R}^n$, and let $p=pt_Lx$. The coordinates of the vector $p$ are called _principal components_ of $x$ \n",
    "\n",
    "_They are more or less express vector $X$ in terms of only our new variables. These new variables corresponding to these vectors $V^1, V^r$ are our $A$. These are new variables which are more or less precisely express all elements of our sample._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The new coordinates via principal components__\n",
    "\n",
    "Let $x \\in \\mathbb{R}^n$, and let $p=pr_Lx$. THe coordinates of the vector $p$ in the basis $v = \\{V^1,\\dots,V^r\\}$ are called _principal components_ of $x$.\n",
    "\n",
    "They are the first $r$ coordinates for the least sqaure solution of the system $V_y = x$, \n",
    "that is, the vector $y=(V^TV)^{-1}V^Tx=V^Tx$.\n",
    "\n",
    "So, the principal components of the t-th row $A_t$ of $A$ are the components of the vector\n",
    "$V^TA^T_t=(A_tV)^T$, the t-th row of the matrix $AV$.\n",
    "\n",
    "Note that $AV=(U\\Sigma V^T)V = U\\Sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([\n",
    "        [-1,-1,0],\n",
    "        [-2,-1,2],\n",
    "        [-3,-2,3],\n",
    "        [1,1,5],\n",
    "    ])\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "print(pca.fir_transform(X))\n",
    "print('the mean row of X is', pca.mean_)\n",
    "print('the singular values are', pca.singular_values_)\n",
    "print('the principal components are', pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be thought of as an unsupervised learning algorithm. The whole process of obtaining principal components from a raw dataset can be simplified in six parts.\n",
    "\n",
    "- Take the whole dataset consisting of d+1 dimensions and ignore the labels such that our new dataset becomes d dimensional.\n",
    "- Compute the mean for every dimension of the whole dataset.\n",
    "- Compute the covariance matrix of the whole dataset.\n",
    "- Compute eigenvectors and the corresponding eigenvalues.\n",
    "- Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d × k dimensional matrix W.\n",
    "- Use this d × k eigenvector matrix to transform the samples onto the new subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the result from PCA?\n",
    "- The direction given by $\\vec{v}_1$ (called the first principal direction) in $\\mathbb{R}^m$ accounts for an amount $\\lambda_1$ of the total variance, $T$. It’s a $\\frac{\\lambda_1}{T}$ fraction of the total variance. Similarly, we can do the same for the second principal direction, $\\vec{v}_2$, and the fraction would be $\\frac{\\lambda_2}{T}$\n",
    "- The vector $\\vec{v}_1 \\in \\mathbb{R}^m$ is the most significant/important direction of the data set.\n",
    "- Among the directions that are orthogonal to $\\vec{v}_1, \\vec{v}_2$ is the most significant direction and similarly, among the directions orthogonal to both $\\vec{v}_1$ and $\\vec{v}_2, \\vec{v}_3$ is the most significant direction and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What exactly has PCA done for the data set?__\n",
    "\n",
    "t has eventually reduced the dimension of the data set. It is often the case that the largest few eigenvalues of S are much greater than the rest. If we assume that we have $m=15$ and total variance $T=110$ such that $\\lambda_1=100.1, \\lambda_2=9.5$ and $\\lambda_3,\\lambda_4,\\dots \\lambda_15$ are all less than 0.1. So even though our data points are forming some impossible to imagine figure in $\\mathbb{R}^{15}$. PCA tells us that these points cluster only near a 2D plane spanned by $\\vec{v}_1$ and $\\vec{v}_2$. To be more precise, the data points would be clustered around a plane passing through $\\mu$ and spanned by orthogonal vectors $\\vec{v}_2$. data point will look like a rectangular strip inside the plane. Therefore, we have reduced the problem from 15 to 2 dimensional which is in some sense easier to visualize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

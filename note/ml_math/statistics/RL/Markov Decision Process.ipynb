{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Markov Process and Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov process. A sequence of states is Markov if and only if the\n",
    "probability of moving to the next state $S_{t+1}$ depends only on the present state St and not on the\n",
    "previous states $S_1, S_2, \\dots , S_{t−1}$. That is, for all $t$,\n",
    "\n",
    "$\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, S_2, \\dots, S_t]$\n",
    "\n",
    "We always talk about time-homogeneous Markov chain in RL, in which the probability of the\n",
    "transition is independent of $t$:\n",
    "\n",
    "$\\mathbb{P}[S_{t+1} = s^{\\prime}|S_t = s] = \\mathbb{P}[St = s^{\\prime}|S_{t−1} = s].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally,\n",
    "\n",
    "__Definition 1__ (Markov Process). a Markov Process (or Markov Chain) is a tuple (S,P), where\n",
    "\n",
    "- $\\mathcal{S}$ is a finit set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix. $P_{ss^{'}} = \\mathbb{P}[S_{t+1}=s^{'}|S_t=s]$ \n",
    "\n",
    "The dynamics of the Markov process proceeds as follows: We start in some state $s_0$, and moves to some successor state $s_1$. drawn from $P_{s_0s_1}$. We then moves to $s_2$ drawn from $P_{s_1s_2}$ and so on. We represent this dynamic as follows:\n",
    "\n",
    "$s_0 \\to s_1 \\to s_2 \\to s_3 \\to \\cdots$\n",
    "\n",
    "If we introduce reward, action and discount into a Markov process, we get a Markov decision process.\n",
    "\n",
    "__Definition 2__ (Markov Decision Process). A Markov decision process is a tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\gamma, \\mathcal{R})$, where:\n",
    "\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{A}$ is a finite set of actions\n",
    "- $\\mathcal{P}$ is the state transition probability matrix $\\mathcal{P}^a_{ss^{'}} = \\mathbb{P}[S_{t+1} = s^{'}|S_t=s,A_t=a]$\n",
    "- $\\gamma$ is called the discount factor.\n",
    "- $\\mathcal{R} : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$ is a reward function.\n",
    "\n",
    "The MDP is used to model the environment in reinforcement learning. In the MDP, the transition to the next state $S_{t+1}$ depends not only on the current state $S_t$, but also depends on the action\n",
    "$A_t$ you make at the current state. Also, each state-action pair is attached with a reward function.  \n",
    "\n",
    "The dynamic of MDP proceeds as follows: We start in some state $s_0$, and choose some actions\n",
    "$a_0 ∈ A$ to take in the MDP. As a result of our choice, the state of the MDP randomly transits to\n",
    "some successor state $s_1$, drawn from $P^{a_0}_{s_0s_1}$.  \n",
    "Then, from state $s_1$, we pick another action $a_1$. Again, we come to some state $s_2$, drawn from $P^{a_1}_{s_1s_2}$. We then pick $a_2$, and so on. We can represent this sequential decision making process as follows:\n",
    "\n",
    "$s_0 \\to^{a_0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Return, Policy and Value function\n",
    "\n",
    "Our goal in RL is to choose actions over time so as to maximize the expected value of the return,\n",
    "i.e. choose the optimal policy. We define return and policy as follows. The return $G_t$ is the total\n",
    "discounted reward from time-step $t$.\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum^{\\infty}_{k=0}\\gamma^k R_{t+k+1}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

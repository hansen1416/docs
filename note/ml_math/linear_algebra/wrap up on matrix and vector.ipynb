{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, after watching the yesterday webinar I’ve got an understanding of the ‘warp-ups’ importance at some points of the learning process. So, I would like to share with you my wrap up slice at this part of the course but from a little bit different standpoint then yesterday webinar, which was more like “how to”. I’ll try to convey the notion of “why?\" and \"what for?”\n",
    "Ok, two basic concepts that we have got so far are matrices and vectors.\n",
    "I see a vector as pointer in N-dimensional space to state of an object that can be described in terms of its properties. These properties are incorporated in the dimensions of our imaginary micro-world where this vector lives. We can even count them, for that we only need a basis which contains all properties and their measure unit. These properties incorporated in our basis and its dimensions are unique and crucial in some sort. They are linearly independent of each other thus each of them provides some useful information about object’s current state. So, we have that pointer, and if for example our object is not static, but dynamic, then its state vector will change with time and we can draw a nice graph of its movement. Is it enough for us? Usually, not, we can do more.\n",
    "For that we need matrices or in another word - transformation of our initial micro-world to another one. In some sense we can stretch, squeeze, shrink, rotate the space in our world as we see fit, like gods.  This manipulation will cause our vector to change too, of course, because it is part of our world. Generally, to find a solution to system of linear equations, means to find that vector pointing to initial state given the vector of final state which we can observe after applying given transformation. Sometimes it is only one state, sometimes there are a lot of them. More on topic of matrix as transformations in 3blue1brow series of videos Essence of Linear Algebra. Here is an example video, but I strongly encourage you to watch the whole playlist.  \n",
    "https://www.youtube.com/watch?v=kYB8IZa5AuE&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3\n",
    "Next, we have got pointers and world manipulations. What can we do with that and why do we even need to do something? Usually, this is a question of your task, but consider an example when you have a bunch of points of one type of state and bunch of another and you need to split them somehow but they are all mixed. Or you have complicated graph of you observed object state change with time and you want to find some critical point or draw a line dividing levels, but for that you need your graph to be a line for example. That is where matrix transformations become useful. And that most wonderful part is that it is totally reversable process, that is why the inverse matrices exists. They allow you to come back to your initial world state buy applying reversed transformation in reversed order. The order is crucial here. I like example form Gilbert Strang lectures when he says that if you put on you socks a then shoes before going outside, then to reverse this process you need FIRST take off shoes and then the socks.\n",
    "Here are the examples of useful transformations:\n",
    "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif\n",
    "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/topology_2D-2D_train.gif\n",
    "Apparently, not all transformations or matrices are good. Some of them are useless (do nothing), like identity matrix for example, which leaves everything as it was. Some of them are redundant, including linearly dependent parts which makes them non-invertible in that case. Overall, they usually contain much more information then we need to analyze them. And here where the form L/U/etc. - forms come to stage.\n",
    "Basically, all you need to analyze any matrix is its Reduced Row Echelon Form (or RREF). In Soviet Russia we also call it a canonical form. This form will tell you anything you need to know about the matrix, its rank, solutions, etc. To get this you just need to apply Gaussian elimination process and that is all. I imagine  the RREF as hash function that convolutes your matrix to compact representation, which can then be easily analyzed. \n",
    "Point is: struggling with matrix? -> rref it and you are done for most of your purposes. By the way, in python you can easily do it by creating Matrix class from sympy lib, and then call rref() method. You will get tuple of 2 elements, first is rref-matrix, the second are pivots from main diagonal before reduction to 1.\n",
    "The most amusing part of linear algebra for me is how easy it tackles the adding of level of freedom to our task. Remember in Calculus lectures when we go from 1-st order derivatives to the 2-nd ones “our lives become sufficiently harder” as Anton would have said. That is not the case for linear algebra. 1-d, 2-d? Easy. 3, 4,5 ... I don’t’ care N-d? Cakewalk. Of course, it does not mean that its power is unbounded. Our usual tasks that require adding level of freedom are only extra row/ column in matrix which does not makes it much harder to understand. Of course, linear algebra has its own more “higher level” levels of freedoms like 3+-d matrices transformations (hello tensors, we are coming). But even the basic ones allow us to reach the level of abstraction unreachable before. An abstraction that can be interpreted and conveyed, which is of paramount importance. (edited) \n",
    "\n",
    "https://www.youtube.com/watch?v=kYB8IZa5AuE&amp;index=3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

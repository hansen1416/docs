{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Midpoint rule__\n",
    "\n",
    "$ \\int_a^b f(x) dx \\approx (b-a)f(\\frac{a+b}{2}) $\n",
    "\n",
    "__Error__\n",
    "\n",
    "$|Error| < \\frac{M(b-a)^3}{24n^2} $, where n is the number of segments in the uniform tagged partiton and M is maximal absolute value of the second derivative: $ M = sup_{[a,b]}f^{\\prime\\prime}(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle(f, a, b, n):\n",
    "    # your code goes here\n",
    "    step = (b-a)/n\n",
    "    x = [xi for xi in np.arange(a, b+step, step)]\n",
    "\n",
    "    x_r = x[1:]\n",
    "    x.pop()\n",
    "    \n",
    "    return (b-a)/n * sum(f((x+y)/2) for x,y in zip(x,x_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trapezoidal rule__\n",
    "\n",
    "$ \\int_a^b f(x) dx \\approx (b-a)(\\frac{f(a)+f(b)}{2}) $\n",
    "\n",
    "__Error__\n",
    "\n",
    "$|Error| < \\frac{M(b-a)^3}{12n^2} $, where n is the number of segments in the uniform tagged partiton and M is maximal absolute value of the second derivative: $ M = sup_{[a,b]}f^{\\prime\\prime}(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapezoidal(f, a, b, n):\n",
    "    # your code goes here\n",
    "\n",
    "    step = (b-a)/n\n",
    "    x = [xi for xi in np.arange(a, b+step, step)]\n",
    "\n",
    "    fa = x[0]\n",
    "    fb = x.pop()\n",
    "    \n",
    "    return (b-a)/n * ((fa+fb)/2 + sum(f(xi) for xi in x[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Simpson rule__\n",
    "\n",
    "$ \\int_a^b f(x) dx \\approx \\frac{b-a}{6}(f(a)+f(b)+4f(\\frac{a+b}{2})) $\n",
    "\n",
    "__Error__\n",
    "\n",
    "$|Error| < \\frac{M(b-a)^5}{180n^4} $, where n is the number of segments in the uniform tagged partiton and M is maximal absolute value of the second derivative: $ M = sup_{[a,b]}f^{\\prime\\prime\\prime\\prime}(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpson(f, a, b, n):\n",
    "    #your code goes here\n",
    "    if n%2==1:\n",
    "        n=n+1\n",
    "    step = (b-a)/n\n",
    "    x = [xi for xi in np.arange(a, b+step, step)]\n",
    "    \n",
    "    sum = 0\n",
    "    \n",
    "    for j in range(1, floor(n/2+1)):\n",
    "        sum += f(x[2*j - 2]) + 4*f(x[2*j - 1]) + f(x[2*j])\n",
    "    \n",
    "    return sum*step/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Middle point rule is the best to use for the function x^x, because it has an unbounded derivatives, so the constant in the error is quite high.\n",
    "\n",
    "\n",
    "2. The estimated error for e^(ax) coincide with the theorical upper boundary per a.\n",
    "\n",
    "\n",
    "3. The estimated error for sin(x) with theoretical upper boundary per n in most of the cases, but the error goes beyond the boundary when n is higher than 1000 for simpson rule. I think it's because the true decimal value of floats in python is a binary approximation, there is a very small rounding error, when we add too many of them together, the error gets visible, therefore the estimated error goes beyond the theorical upper boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

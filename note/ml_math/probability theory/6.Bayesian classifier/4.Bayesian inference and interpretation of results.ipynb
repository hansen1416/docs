{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayesian inference__\n",
    "\n",
    "$L_1, \\dots, L_l$ - languages\n",
    "\n",
    "$T$ - text $f(T)=(f_1(T), \\dots, f_k(T))$  the frequency of character in dataset\n",
    "\n",
    "$P(f(T)|L_i)$ - likelihood\n",
    "\n",
    "_In Bayesian approach, we believe that this L_i is also chosen randomly before we start generating our text. So we make our model a little bit more complicated by adding one step to it. So we believe that first, we select the language and then we use this language to generate our text and we have some a prior distribution on the set of languages._\n",
    "\n",
    "$P(L_i) = q_i$ a prior probabilities of languages, $i=1,\\dots,l$  \n",
    "\n",
    "$P(L_i | f(T)) = ?$ this is the probobility of the language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Recall Bayes' theorem_\n",
    "\n",
    "$H_1, \\dots, H_l; H_a\\cap H_b = \\varnothing; a\\ne b; H_1\\cup \\dots \\cup H_l = \\Omega$\n",
    "\n",
    "$P(H_i | A) = {\\large \\frac{P(A|H_i) \\times P(H_i)}{\\sum^{l}_{s=1}P(A|H_s) \\times P(H_s)} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(L_i|f(T)) = {\\large \\frac{P(f(T)|L_i) \\times P(L_i)}{\\sum^j_{s=1} P(f(T)|L_s) \\times P(L_s)} }$\n",
    "\n",
    "$i_B = argmax \\; P(L_i | f(T)), i = 1,\\dots,l$\n",
    "\n",
    "_Now you can see that if we just want to select the language which maximizes this value, then you can ignore this denominator and compare only numerators, because for all these values we have the same denominator, it does not depend on i. But if you want this full probability, you have to take into account this denominator. In fact, it can be rather useful because the advantage of Bayesian approach is that it not only gives you some result like which model is better in the sense, but it also gives you this probability._  \n",
    "\n",
    "_You can interpret this probability as a measure of how certain algorithm, how confident it is in its prediction. For example, if you see that the probability of even the best language is not so high, it means that model your algorithm is not too much confident about this answer, and probably you have to investigate it by different tools. This is a crucial advantage of Bayesian methods._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The learning process\n",
    "\n",
    "We switch to batch learning, we receive __all the data at the beginning__ and we have to generate a classifications function.\n",
    "\n",
    "### Risk for binary classification\n",
    "\n",
    "- Binary classification $|\\mathcal y|=2$. A classifier is a function from $\\mathcal X \\to \\mathcal y $\n",
    "\n",
    "- For every $x \\in \\mathcal X$, there exists a true label, denoted $h(x)$. True label = the label that is used for training and testing.\n",
    "\n",
    "__Definition__\n",
    "\n",
    "> Given a true classifier $h$ and a measure $P$ over $\\mathcal X$, the __risk__ of a classifier $f$ is $R(f)=P(f(x) \\ne h(x))$\n",
    "\n",
    "Explicit notation $R_{P, h}(f)=R(f)$\n",
    "\n",
    "- $0 \\le R(f) \\le 1$\n",
    "- Even if $R(f) = 0$, the set $E=\\{x: h(x) \\ne f(x)\\}$ can be large, but $P(E)=0$ (even if the risk of a function zero, it can happen that some inputs are classified wrongly.)\n",
    "- We aim at finding a classifier $f$ with small $R(f)$\n",
    "\n",
    "Let $\\mathcal y = {A,B}$, constant functions $f_1(x)=A$ and $f_2(x)=B$. Determine\n",
    "\n",
    "$R(f_1)+R(f_2) = 1$\n",
    "\n",
    "Becasue $Pr[h(x)=B] + Pr[h(x)=A] = 1$\n",
    "\n",
    "$\\to$ it's easy to find a classifier with risk $\\le 1/2$\n",
    "\n",
    "__example__\n",
    "\n",
    "- $\\mathcal X = \\{1,2,3, \\dots\\} \\;\\;\\;\\; \\mathcal y =\\{0,1\\}$\n",
    "- $h(x) = x \\mod 2$\n",
    "- P(x) = 2^{-x}\n",
    "\n",
    "$f(x)=(x \\mod 2) \\cdot [x \\le 10] = \\begin{cases} x \\mod 2 \\;\\; if \\; x\\le 10 \\\\ 0 \\;\\;\\;\\; otherwise\\end{cases}$\n",
    "\n",
    "What is $R(f)$?\n",
    "\n",
    "$2^{-11} + 2^{-13} + 2^{-15} + \\cdots = 2^{-11} \\cdot \\frac{1}{1-1/4} = 1/3 \\cdot 2^{-9}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning process\n",
    "\n",
    "The input of a learning algorithm is a labelled dataset\n",
    "\n",
    "$Z = [(x_1, y_1), \\dots, (x_n, y_n)] \\in (\\mathcal X \\times \\mathcal y)^*$\n",
    "\n",
    "Its output is a classifier $f_S: \\mathcal X \\to \\mathcal y$\n",
    "\n",
    "measure $P$ over $\\mathcal X$   \n",
    "true classifier $h: \\mathcal X \\to \\mathcal y$   \n",
    "interger n\n",
    "\n",
    "- Sample $n$ inputs i.i.d from $P: S=[X_1, \\dots, X_n]$\n",
    "- The learner gets\n",
    "\n",
    "$S^h = [(X_1, h(X_1)), \\dots,(X_n, h(X_n))]$ ($h$ is true function)\n",
    "\n",
    "- Run the learning algorithm and obtain the classifier $f_{S^h}$\n",
    "- Sample a test point $\\tilde{X}$ from $P$. Check $f_{S^h}(\\tilde{X}) \\ne h(\\tilde{X})$\n",
    "\n",
    "Given $P$ and $h$, the performance of the learning algorithm is \n",
    "\n",
    "<center>$P^{n+1}[f_{S^h}(\\tilde{x})\\ne h(\\tilde{x})] = \\mathbb{E}_S R(f_{S^h})$</center>\n",
    "\n",
    "(because we have $n+1$ i.i.d. vatriables taken from $P$ and variables are in $S$ and one variable is $\\tilde{X}$  the the test point. We can also write it as the expected value over choosing $S$ of the learned classifier.)\n",
    "\n",
    "(the smaller This probability, the better for us. If it's close to one. We're also very happy because then we can flip the sign off all classifier generated by the learning algorithm.)\n",
    "\n",
    "- Smaller = better. If close to 1 $\\implies$ invert $f_{S^h}$\n",
    "- For a random selection of $f_+$ and $f_-$: this equal to 1/2\n",
    "\n",
    "---\n",
    "__example__\n",
    "\n",
    ">The learning algorithm may be a probabilistic algorithm. In this case, this internal randomness is also included in the expectation. \n",
    ">\n",
    ">Classifiers are defined as functions, and hence always deterministic. Note that after a learning algorithm has produced a classifier ff and after generating $\\tilde{x} $, evaluating $f(\\tilde{x})$ does not require randomness. \n",
    ">\n",
    ">Let $\\mathcal Y = \\{\\texttt{A}, \\texttt{B}\\}, f_1(x) = \\texttt{A}$ and $f_2(x) = \\texttt{B}$. What is the average risk of the learning algorithm that outputs $f_1$ and $f_2$ with probability 1/2? \n",
    "\n",
    "1/2. We already proved that $\\mathrm{R}(f_1) + \\mathrm{R}(f_2) = 1$ and this is twice the expected value of the risk.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__performance 1-nearest neighbor__\n",
    "\n",
    "$\\mathcal X = \\{-1, 0, +1\\} \\;\\;\\;\\; \\mathcal y = \\{0,1\\}$\n",
    "\n",
    "$D$ is uniform over $\\mathcal X \\;\\;\\;\\; h(x)=x^2 \\;\\;\\;\\; n=2$\n",
    "\n",
    "$1 \\;\\;\\; 0 \\;\\;\\; 1$  \n",
    "$\\bullet \\;\\;\\; \\bullet \\;\\;\\; \\bullet$\n",
    "\n",
    "Cases\n",
    "\n",
    "- $S$ contains 2 different points.(P = 1/3 x 1/3 x 3 = 6/9)  \n",
    "    $\\implies $ 1 point is not in $S$. Always classified wrong  \n",
    "    $\\implies R(f_{S^h}) = 1/3$  \n",
    "    \n",
    "- $S$ contains 2 coinciding points.  \n",
    "    - Point on the side $\\implies R(f_{S^h}) = 1/3$ (P=1/3 x 1/3 x 2 = 2/9)  \n",
    "    - Point on the middle $\\implies R(f_{S^h}) = 2/3$ (P=1/3 x 1/3 x 1 = 1/9)  \n",
    "    \n",
    "Average risk = 1/9 x 2/3 + 8/9 x 1/3 = 10/27 (performance measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-trivial bounds require assumptions\n",
    "\n",
    "fix $\\mathcal X$ (assume $|\\mathcal y| = 2$)\n",
    "\n",
    "__Theorem__\n",
    "> For every learning algorithm, distribution $P$ over $\\mathcal X$, integer $n$, there exists a function $h: \\mathcal X \\to \\mathcal y$ such that\n",
    ">\n",
    ">$P^{n+1}(f_{S^h}(X) \\ne h(X)|X \\notin S) \\ge \\frac{1}{2}$\n",
    "\n",
    "(Probability of predict wrong label on test data is more than 1/2. This implies that when we look to worst case  average risk, it implies that simple reproductive learning is optimal.)\n",
    "\n",
    "_Proof_. Probabilistic method: select $h$ randomly\n",
    "\n",
    "(We will select a random function h, and we will prove the claim for a random function $h$. We show that an expected value of the quantity in the theorem is precisely equal to 1/2. And if the expected value of some random variable it's 1/2, then there must exist a value in the set for which the expression is at least 1/2.)\n",
    "\n",
    "$\\mathbb{E}_h P^{n+1}(f_{S^h}(X) \\ne h(X) | X \\notin S) = \\frac{1}{2}$\n",
    "\n",
    "So all what we need to do is prove above equation.\n",
    "\n",
    "Fix $S, x\\notin S$ and $h$ in $X\\backslash\\{x\\}$ \n",
    "\n",
    "(Select $h$ randomly, for each value, we flip a coin and select the label randomly. Imagine we have already selected all values of $h$ except for the point $x$.)\n",
    "\n",
    "$Pr_{h(x)}[f_{S^h}(x) \\ne h(x)] = \\frac{1}{2}$\n",
    "\n",
    "(In above equation, only thing not fixed is $h(x)$. The outcome of the learning algorithm is fixed, it's prediction on $x$ is fixed. So above probability is 1/2, two labels, select randomly)\n",
    "\n",
    "If above statement is true, it is also true for a random $S$, only requirement is $X \\notin S$\n",
    "\n",
    "$Pr_{h, S, X \\notin S}[f_{S^h}(x) \\ne h(x)] = \\frac{1}{2}$\n",
    "\n",
    "$\\implies Pr_{h, S, X}[f_{S^h}(x) \\ne h(x) | X \\notin S] = \\frac{1}{2}$\n",
    "\n",
    "If we write this equation as expected value $\\implies \\mathbb{E}_h P^{n+1}(f_{S^h}(X) \\ne h(X) | X \\notin S) = \\frac{1}{2}$\n",
    "\n",
    "---\n",
    "If the learning algorithm is probabilistic, we could fix its source of randomness to a certain value, and apply the reasoning for a deterministic algorithm. However, at 2m15 and 2m26, we also take the probability over this source of randomness. How does this possibly affect the value of the probability?\n",
    "\n",
    "__The probability always remains equal to 1/2.__ for random learners, the proof is essentially the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Corollary__\n",
    "\n",
    "> Let $P$ be uniform. There exists a function $h: \\mathcal X \\to \\mathcal y$ such that $ \\mathbb E R(f_{S^h}) \\ge \\frac{1}{2}(1-\\frac{n}{|\\mathcal X|})$\n",
    "\n",
    "($n$ is the size of trainning set)\n",
    "\n",
    "_Proof_\n",
    "\n",
    "Since $P$ is uniform, $\\implies P(S) \\le \\frac{n}{| \\mathcal X|}$ (It can be smaller if $S$ has repeated elements.)\n",
    "\n",
    "Let $A = [f_{S^h}(x) \\ne h(x)], B = [x \\notin S]$ \n",
    "\n",
    "(A equal to the event that our prediction is wrong, and B equal to the event that we pick a test point outside the set)\n",
    "\n",
    "Bayes law $Pr[A]\\ge Pr[A \\& B] = Pr[A|B]\\cdot Pr[B] \\ge \\frac{1}{2} \\cdot (1-\\frac{n}{|\\mathcal X|})$\n",
    "\n",
    "The corollary is related to the curse of dimensionality.\n",
    "\n",
    "If $\\mathcal X$ is infinite, e.g. $\\mathcal X = \\mathbb R$: apply to an arbitrary large finite subset $\\mathcal X^{'} \\subseteq \\mathcal X$. (we conclude that a learning algorithm has risk at least 1/2, arbitrarily close to 1/2. Since $\\frac{n}{|\\mathcal X|} \\to 0$)\n",
    "\n",
    "If $\\mathcal X = \\{0,1\\}^p$, then to do significantly better than 1/2, we need $\\Omega(2^p)$ samples. ($|\\mathcal X| = 2^p$)\n",
    "\n",
    "__Conclusion:__\n",
    "\n",
    "If any function can be true function, $\\to$ reproductive learning is optimal. But this is infeasible for large dimensions.  \n",
    "(if we want to have an interesting learning algorithm and interesting performance bounds, we need some assumptions on the class $h$. Without assumptions, we might need exponentially large training set sizes on $p$ dimensional inputs. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogue results for multiclass classification\n",
    "\n",
    ">In the movie we considered binary classification, i.e., $|\\mathcal Y| = 2$. We showed that for each input space $\\mathcal X$, for each learning algorithm and each integer $n$, there exists a true classifier $h : \\mathcal X \\rightarrow \\mathcal Y$ for which \n",
    ">\n",
    ">$P^{n+1}(f_{S^h}(X) \\ne h(X)|X \\notin S) \\ge 1/2$\n",
    ">\n",
    ">Now consider multiclass classification, i.e., we assume $|\\mathcal Y| \\ge 2$. Let $k = |\\mathcal Y|$. If we apply the same reasoning as in the proof, we obtain the same inequality above with a possibly different value in the right-hand side. This value is larger if $k \\ge 3$.\n",
    ">\n",
    ">What is this value?\n",
    "\n",
    "$1-1/k$ \n",
    "\n",
    "(Probability of $h(X) = f_{S^h}(X)$ is $1/k$)\n",
    "\n",
    ">In the movie also a corollary is proven that concerns the uniform distribution. What is the analogous lower bound for multiclass classification with $|\\mathcal Y| = k \\ge 2$? Let $s = |\\mathcal X|$ and as usual, let $n$ be the size of the training set.\n",
    "\n",
    "$(1-1/k)(1-n/s)$\n",
    "\n",
    "Same as proof of corallary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing average risk for large training sets\n",
    "\n",
    "Define the classes $\\mathcal H$ with vanishing worst-case risk.\n",
    "\n",
    "- Let $\\mathcal H$ be a set of classifiers $f: \\mathcal X \\to \\mathcal y$. Recall $|\\mathcal H| \\ge 1$ and $|\\mathcal y| = 2$\n",
    "- We will prove risk bounds that hold for\n",
    "    - all true classifier $h$ in $\\mathcal H$\n",
    "    - we do not restrict the distribution $P$ over $\\mathcal X$\n",
    "    \n",
    "__Definition__\n",
    "\n",
    ">A class $\\mathcal H$ is learnable with vanishing worst-case risk if there exists a learning algorithm for which\n",
    ">\n",
    ">$\\lim_{n \\to +\\infty} \\text{sup}_{h \\in \\mathcal H, P} P^{n+1}(f_{S^h}(X) \\ne h(X)) = 0$\n",
    "\n",
    "(if there exists a learning algorithm for which the supreme over all distributions $P$. And all true classifiers $h \\in \\mathcal  H$ has an error probability arbitrary close to zero when $n$ becomes large.)\n",
    "\n",
    "- $|\\mathcal H|=1 \\implies \\mathcal H$ is learnable\n",
    "\n",
    "(he trivial class that contains only one classifier is of course, learnable. We simply considered the learning algorithm that outputs the single tone element in H.)\n",
    "\n",
    "- $|\\mathcal X| = \\infty$ and $\\mathcal H$ contains all functions on $\\mathcal X \\implies \\mathcal H$ is not learnable.\n",
    "\n",
    "Let $\\mathcal X^{'} \\subseteq X$ of size \n",
    "\n",
    "---\n",
    "\n",
    "Examples of classes that do and do not have this property.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

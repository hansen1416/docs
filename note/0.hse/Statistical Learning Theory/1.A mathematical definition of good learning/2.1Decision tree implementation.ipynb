{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees for detecting heart disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will train a simple binary decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             # to load and manipulate data\n",
    "import numpy as np              # to manipulate with vectors and matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2      3      4    5    6      7    8    9    10   11   12  13\n",
       "0  63.0  1.0  1.0  145.0  233.0  1.0  2.0  150.0  0.0  2.3  3.0  0.0  6.0   0\n",
       "1  67.0  1.0  4.0  160.0  286.0  0.0  2.0  108.0  1.0  1.5  2.0  3.0  3.0   2\n",
       "2  67.0  1.0  4.0  120.0  229.0  0.0  2.0  129.0  1.0  2.6  2.0  2.0  7.0   1\n",
       "3  37.0  1.0  3.0  130.0  250.0  0.0  0.0  187.0  0.0  3.5  3.0  0.0  3.0   0\n",
       "4  41.0  0.0  2.0  130.0  204.0  0.0  2.0  172.0  0.0  1.4  1.0  0.0  3.0   0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data', \n",
    "                  header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By information on the dataset page https://archive.ics.uci.edu/ml/datasets/Heart+Disease we have the following variables: \n",
    "- **age**,\n",
    "- **sex**, 1 -- male, 0 -- female;\n",
    "- **cp**, chest pain type: \n",
    "    - Value 1: typical angina, \n",
    "    - Value 2: atypical angina\n",
    "    - Value 3: non-anginal pain\n",
    "    - Value 4: asymptomatic \n",
    "- **restbp**, resting blood pressure (in mm Hg)\n",
    "- **chol**, serum cholesterol in mg/dl\n",
    "- **fbs**, fasting blood sugar\n",
    "- **restecg**, resting electrocardiographic results\n",
    "- **thalach**,  maximum heart rate achieved\n",
    "- **exang**, exercise induced angina\n",
    "- **oldpeak**, ST depression induced by exercise relative to rest\n",
    "- **slope**, the slope of the peak exercise ST segment.\n",
    "- **ca**, number of major vessels (0-3) colored by fluoroscopy\n",
    "- **thal**, this is short of thalium heart scan.\n",
    "- **hd**, diagnosis of heart disease, the predicted attribute: 0 for no diagnozed heart disease.\n",
    "\n",
    "The good idea is to rename columns in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the column numbers to column names\n",
    "df.columns = ['age', \n",
    "              'sex', \n",
    "              'cp', \n",
    "              'restbp', \n",
    "              'chol', \n",
    "              'fbs', \n",
    "              'restecg', \n",
    "              'thalach', \n",
    "              'exang', \n",
    "              'oldpeak', \n",
    "              'slope', \n",
    "              'ca', \n",
    "              'thal', \n",
    "              'hd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to distinguish presence (values 1,2,3,4) from absence (value 0) of heart disease, that corresponds to the value of the last column. It will be our **target** variable.\n",
    "\n",
    "To make a simple model, we do not require to use all the data. To simplify, we suggest the following binary variables of our interst:\n",
    "* age is less or equal than 40;\n",
    "* age is greater than 60;\n",
    "* sex is male (0 corresonds to female);\n",
    "* chest pain is not asymptomatic (in other words, patient has a chest pain);\n",
    "* is fasting blood sugar level to high;\n",
    "* resting electrocardiographic results are normal;\n",
    "* has exercise induced angina;\n",
    "* resting blood pressure greater or equal than 130 (corresponds to hypertension);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    df['age'] <= 40, \n",
    "    df['age'] > 60, \n",
    "    df['sex'], \n",
    "    df['cp'] != 4, \n",
    "    df['fbs'], \n",
    "    df['restecg'] == 0,\n",
    "    df['exang'],\n",
    "    df['restbp'] >= 130\n",
    "]).T ## We transpose the matrix because numpy generates a matrix with selected rows not columns as we needed\n",
    "y = np.array(df['hd'] > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to write our own decision tree by implementing the greedy algorithm discussed on the lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we want to compute Gini impurity for a given leaf and for a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity_leaf(y):\n",
    "    '''\n",
    "    Find a value of gini impurity for a given leaf\n",
    "    :param y: true binary labels correponds to this leaf\n",
    "    :return gini: value of gini impurity\n",
    "    '''\n",
    "    # Do not forget that y can be empty! In this case use any value.\n",
    "    if y.shape[0] == 0:\n",
    "        return 0.5\n",
    "    p1 = np.sum(y == 1) / y.shape[0]\n",
    "    return 1 - (1-p1)**2 - p1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity_node(feature, y):\n",
    "    '''\n",
    "    Find a value of gini impurity for a given node that corresponds to the feature\n",
    "    :param feature: binary feature vector corresponds to this node\n",
    "    :param y: true binary labels correponds to this leaf\n",
    "    :return gini:\n",
    "    '''\n",
    "    p = np.sum(feature == 1) / feature.shape[0]\n",
    "    return p * gini_impurity_leaf(y[feature == 1]) + (1-p) * gini_impurity_leaf(y[feature == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_feature(X,y):\n",
    "    '''\n",
    "    Find the best feature given dataset X and true labels y\n",
    "    :param X: dataset matrix, rows corresponds to samples, colums to features\n",
    "    :param y: true binary labels\n",
    "    :return i: index that corresponds to feature column with least gini impurity\n",
    "    :return gini: corresponding value of gini impurity\n",
    "    '''\n",
    "    best_feature = None\n",
    "    best_gini = None\n",
    "    for feature in range(X.shape[1]):\n",
    "        gini = gini_impurity_node(X[:, feature], y)\n",
    "        if best_gini is None or gini < best_gini:\n",
    "            best_feature = feature\n",
    "            best_gini = gini\n",
    "    return best_feature, best_gini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement our own decision tree, using the functions above. We will use trees with fixed depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth):\n",
    "        self._tree = {}\n",
    "        self._max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._fit_node(X,y, self._tree, 0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        # We make predictions of each object\n",
    "        for x in X:\n",
    "            predicted.append(self._predict_node(x, self._tree))\n",
    "        return np.array(predicted)\n",
    "    \n",
    "    def _fit_node(self, sub_X, sub_y, node, depth):\n",
    "        if depth == self._max_depth or np.sum(sub_y == 1) == 0 or np.sum(sub_y == 0) == 0:\n",
    "            # It is a leaf node! Use the most common value of sub_y here as a \"value\"\n",
    "            node[\"type\"] = \"leaf\"\n",
    "            node[\"value\"] = int(2 * np.sum(sub_y) > sub_y.shape[0])\n",
    "            return\n",
    "        # Otherwise it is an ordinary node. \n",
    "        node[\"type\"] = \"node\"\n",
    "        node[\"feature\"] = find_best_feature(sub_X, sub_y)[0]\n",
    "        node[\"left_child\"] = {}\n",
    "        node[\"right_child\"] = {}\n",
    "        # Recusive call to fill the rest of tree\n",
    "        zero_mask = (sub_X[:, node[\"feature\"]] == 0)\n",
    "        one_mask = (sub_X[:, node[\"feature\"]] == 1)\n",
    "        self._fit_node(sub_X[zero_mask], sub_y[zero_mask], node[\"left_child\"], depth+1)\n",
    "        self._fit_node(sub_X[one_mask], sub_y[one_mask], node[\"right_child\"], depth+1)\n",
    "    \n",
    "    def _predict_node(self, x, node):\n",
    "        if node[\"type\"] == \"leaf\":\n",
    "            return node[\"value\"]\n",
    "        feature = node[\"feature\"]\n",
    "        # Left child corresponds to 0-value, right child corresponds to 1-value\n",
    "        next_node = None\n",
    "        if x[feature] == 0:\n",
    "            next_node = node[\"left_child\"]\n",
    "        else:\n",
    "            next_node = node[\"right_child\"]\n",
    "        return self._predict_node(x, next_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use this tree for classification of heart disease and test our predictions using sklearn library. We will use very simple tree of depth 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x7f2a2792f610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dt = DecisionTree(max_depth=2)\n",
    "clf_dt.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the accuracy on a given dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-51c215125e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(clf_dt.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might argue that traing and testing on the same dataset is not fair. So, we need to split it to two different datasets and test it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = DecisionTree(max_depth=2)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "accuracy_score(clf_dt.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows us that the model does not overfit. The reason is clear -- the model is too simple and our features are too simple. Overfitting is pretty common for deep neural networks and huge decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advice**: play around with the code. Playing with the code is the best way to learn from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

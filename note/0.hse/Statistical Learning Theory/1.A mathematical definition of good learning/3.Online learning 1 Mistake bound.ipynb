{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The online mistake model\n",
    "\n",
    "Hypothesis set $\\mathcal H$ containing function $f: \\mathcal X \\to \\mathcal y$ (assume one of the function always predict correctly)\n",
    "\n",
    "Supervisor has $h \\in \\mathcal H$\n",
    "\n",
    "Input sequence $x_1,x_2, \\dots \\in \\mathcal X$\n",
    "\n",
    "For increasing $t$, the online algorithm:\n",
    "\n",
    "- receives an input $x_t$,\n",
    "- makes a prediction $\\hat{y}_t$,\n",
    "- receives the correct label $y_t = h(x_t)$\n",
    "\n",
    "Goal: minimize the number of wrong predictions.\n",
    "\n",
    "---\n",
    "_source: svivek_\n",
    "\n",
    "### Mistake bound algorithms\n",
    "\n",
    "- Setting:\n",
    "    - Instance space: $\\mathcal X$ (domensionality $n$)\n",
    "    - Target $f: \\mathcal X \\to \\{0,1\\}, f \\in C$ the concept class (parameterized by $n$)\n",
    "- Learning protocol:\n",
    "    - Leaner is given $x\\in \\mathcal X$, randomly chosen\n",
    "    - Leaner predict $h(x)$ and is then given $f(x) \\gets $ _the feedback_\n",
    "- Performance: learner makes a mistake when $h(x)\\ne f(x)$\n",
    "    - $M_A(f,S)$: number of mistakes algorithm $A$ makes on sequence $S$ of examples for the target function $f$\n",
    "    - $M_A(C)= max_{f\\in S} M_A(f,S)$: The maximum possible number of mistakes made by $A$ for any target function in $C$ and any sequence $S$ of examples\n",
    "    \n",
    "### Learnability in the mistake bound model\n",
    "\n",
    "- __Algorithm $A$ is a _mistake bound algorithm_ for the concept class $C$ if $M_A(C)$ is a polynomial in the dimensionality $n$.__\n",
    "    - That is, the maximum number of mistakes it makes for any sequence of inputs (perhaps even an adversarially chosen one) is polynomial in the dimensionality\n",
    "    \n",
    "- __A concept class is learnable in the mistake bound model if there exists an algorithm that makes a polynomial number of mistakes for any sequence of examples__\n",
    "    - Polynomial in the dimensionality of the examples\n",
    "    \n",
    "– Some things to think about:  \n",
    "• What is the relation to the “real” goal? What is the real goal of learning?  \n",
    "• Does online learning generate a hypothesis that does well onpreviously unseen data?  \n",
    "\n",
    "_end source: svivek_\n",
    "\n",
    "---\n",
    "\n",
    "### Example: $\\mathcal X = {1,0}^p$\n",
    "\n",
    "Let $f_i(x^{'}) = x^{'}_i$, for $i=1, \\dots, p$ and $\\mathcal H = \\{f_1, f_2, \\dots, f_p \\}$\n",
    "(inputs are bit string of length $p$, hypothesis class $f_1, f_2, \\dots, f_p$)\n",
    "\n",
    "__Algorithm:__ on input $x_t$ predict $f_i(x_t)=x_{t,i}$ for the smallest $i$ such that $y_1=f_i(x_1),\\dots,y_{t-1} = f_i(x_{t-1})$\n",
    "\n",
    "(we will look to the smallest $i$, such that $f_i$ is consistent with all previously received labels.)\n",
    "\n",
    "---\n",
    "\n",
    "Example: $\\mathcal X = \\{0, 1\\}^3, p=3\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$    Assume true function: $f_3$\n",
    "\n",
    "Receive: 010$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$consistent $\\{f_1, f_2, f_3\\}$  \n",
    "Predict: $f_1(010)=0$ __correct__  \n",
    "Receive: $y_1=0$$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ The set of function that is still consistent $\\{f_1, f_3\\}$  \n",
    "\n",
    "Receive: 110  \n",
    "Predict: $f_1(110)=1$ __wrong__  \n",
    "Receive: $y_2=0$$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ The set of function that is still consistent $\\{f_3\\}$ \n",
    "\n",
    "At this point, only $f_3$ is consistent with the received labels so far. That means from this point on, we will always predict correct because we choose a true function $h$, to $f_3$\n",
    "  \n",
    "---\n",
    "  \n",
    "Example: $\\mathcal X = \\{0, 1\\}^3, p=3\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$    Assume true function: $f_3$\n",
    "\n",
    "Receive: 011$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$consistent $\\{f_1, f_2, f_3\\}$  \n",
    "Predict: $f_1(011)=0$ __wrong__  \n",
    "Receive: $y_1=0$$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ The set of function that is still consistent $\\{f_1, f_3\\}$  \n",
    "\n",
    "Receive: 001  \n",
    "Predict: $f_1(001)=0$ __wrong__  \n",
    "Receive: $y_2=1$$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ The set of function that is still consistent $\\{f_3\\}$ \n",
    "\n",
    "At this point, only $f_3$ is consistent with the received labels so far. That means from this point on, we will always predict correct because we choose a true function $h$, to $f_3$\n",
    "\n",
    "How many mistakes on a sequence $x_1, x_2, x_3, \\dots$? If true function classifier is $f_1$, then always 0 mistakes.\n",
    "\n",
    "$p-1$\n",
    "\n",
    "because each time we make a mistake, we delete at least one function of the list of consistent functions. Also, we can achieve $p - 1$ errors\n",
    "\n",
    "### More generally for all $\\mathcal X$ and finite $\\mathcal H$\n",
    "\n",
    "Let $\\mathcal H = \\{f_1,\\dots,f_K\\}$\n",
    "\n",
    "Algorithm: Predict according to $f_i$ for the smallest $i \\le K$ such that $f_i$ is consistent with all previously seen data.\n",
    "\n",
    "$\\to$ makes at most $K-1$ mistakes\n",
    "\n",
    "### Number of mistakes\n",
    "\n",
    "__Definition__\n",
    "\n",
    ">Given a hypothesis class $\\mathcal H$ and an algorithm $A$, the _maximum number of mistakes_ $M(\\mathcal H, A)$ of $A$ on $\\mathcal H$ is $max\\{\\#\\text{mis}(A,f,x_1x_2\\cdots):f \\in \\mathcal H$ and $x_1,x_2,\\dots \\in \\mathcal X\\}$, where $\\text{mis}(A,f,x_1x_2\\cdots)$ is the number of mistakes made by $A$ on $x_1,x_2,\\dots$ if $f$ is true. \n",
    "\n",
    "For every finite $\\mathcal H$, there exists an algorithm $A$ for which $M(A, \\mathcal H) \\le |\\mathcal H| - 1$\n",
    "\n",
    "---\n",
    "_source: svivek_\n",
    "\n",
    "### Generic Mistake Bound Algorithms\n",
    "\n",
    "- Let $C$ be a finite concept class\n",
    "- Goal: Learn $f \\in C$\n",
    "\n",
    "- Algorithm __CON__ (short for consistent):\n",
    "    In the $i^{th}$ stage of the algorithm:\n",
    "    - $C_i$ = all concepts in C consistent with all $i – 1$ previously seen examples\n",
    "    - Choose randomly $f \\in C_i$ and use it to predict the next example\n",
    "\n",
    "- It is not hard to show that $C_{i+1}\\subseteq C_i$\n",
    "- If a mistake is made on the $i^{th}$ example, then $|C_{i+1}| < |C_i|$, progress is made\n",
    "- The __CON__ algorithm makes at most $C − 1$ mistakes\n",
    "\n",
    "_end source: svivek_\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "__Theorem__\n",
    "\n",
    "> For every finite $\\mathcal H$, there exists an algorithm $A$ for which $M(A, \\mathcal H) \\le \\log_2|\\mathcal H|$\n",
    "\n",
    "_Proof_. For a list $L \\in \\mathcal y^*$ of binary labels, let $maj(L)$ denote a label appearing at least $|L|/2$ times. e.g.$maj([1,1,0,0,1]) = 1$\n",
    "\n",
    "__Halving algorithm__\n",
    "\n",
    ">$\\mathcal F_1 \\gets \\mathcal H$\n",
    ">\n",
    ">while True:  \n",
    ">    $\\;\\;\\;\\;$Receive $x_t$  \n",
    ">    $\\;\\;\\;\\;$Predict $\\hat{y}_t = maj([f(x_t): f\\in \\mathcal F_t])$  \n",
    ">    $\\;\\;\\;\\;$Receive $y_t$  \n",
    ">    $\\;\\;\\;\\;$$\\mathcal F_{t+1} \\gets \\{f\\in \\mathcal F_t : f(x_t)=y_t\\}$\n",
    "\n",
    "Makes at most $\\log_2|\\mathcal H|$ mistakes, because \n",
    "- if $\\hat{y}_t \\ne y_t$, then $|\\mathcal F_{t+1}| \\le \\frac{1}{2}|\\mathcal F_{t}|$\n",
    "- thus after $m$ mistakes, we have $|\\mathcal F_{t}| \\le |\\mathcal H| \\cdot 2^{-m}$\n",
    "- the true hypothesis $f$ always remains in $\\mathcal F_t$ thus \n",
    "\n",
    "$1\\le |\\mathcal F_{t}| \\le |\\mathcal H| \\cdot 2^{-m} \\implies m \\le \\log_2|\\mathcal H|$\n",
    "\n",
    "---\n",
    "\n",
    "_source: svivek_\n",
    "\n",
    "### The Halving Algorithm\n",
    "\n",
    "- Let $C$ be a finite concept of class\n",
    "- Goal learn $f \\in C$\n",
    "\n",
    "Procedure:\n",
    "\n",
    "- Initialize $C_0 = C$, the set of all possible functions\n",
    "- When an example $x$ arrives:\n",
    "    - Predict the label for $x$ as 1 if a majority of the functions in $C_i$, predict 1. Otherwise 0. That is, output = 1 if\n",
    "    $|\\{h(x)=1: h \\in C_i\\}| > |\\{h(x) = 0: h \\in C_i\\}|$\n",
    "    \n",
    "    - If prediction $\\ne f(x)$\n",
    "        - Update $C_{i+1}=$ all elements of $C_i$ that agree with $f(x)$\n",
    "- Learning ends when there is only one element in $C_i$\n",
    "\n",
    "### How many mistakes will the Halving algorithm make?\n",
    "\n",
    "Suppose it makes n mistakes. Finally, we will have the final set of concepts $C_n$ with one element.\n",
    "\n",
    "$C_n$ was created when a majority of the functions in $C_{n-1}$ were incorrect\n",
    "\n",
    "$1 = |C_n| < \\frac{1}{2}|C_{n-1}| < \\frac{1}{2} \\cdot \\frac{1}{2} |C_{n-2}| < \\dots < \\frac{1}{2^n}|C_0| = \\frac{1}{2^n}|C|$\n",
    "\n",
    "$\\implies |C| > 2^n \\implies n < log_2|C|$\n",
    "\n",
    "### Summary: The Halving algorithm\n",
    "\n",
    "- A simple algorithm for finite concept spaces  \n",
    "    - Stores a set of hypotheses that it iteratively refines  \n",
    "        - Receive an input  \n",
    "        - Prediction: the label of the majority of hypotheses still under consideration  \n",
    "        - Update: If incorrect, remove all inconsistent hypotheses  \n",
    "        \n",
    "- Makes $O(\\log|C|)$ mistakes for a concept class $C$  \n",
    " \n",
    "- Not always optimal because we care about minimizing the number of mistakes in the future!  \n",
    "    - What if, instead of eliminating functions that disagree with this example, we down-weight them  \n",
    "    - Perhaps via multiplicative or additive updates…  \n",
    "\n",
    "_end source: svivek_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__example__\n",
    "\n",
    ">Often, $|\\mathcal H|$ is very large. For example it equals $10^{15}$ for binary decision trees with 8 inputs and 15 nodes. In this case, it is very time-consuming to compute the majority vote exactly, and one might use an approximate algorithm.\n",
    ">\n",
    ">Assume that our approximate algorihm calculates the majority vote correctly if at least a fraction $1/\\sqrt{2} \\sim 0.707$ of the classifiers in $\\mathcal F_{t}$ produce the same label for the given input $x_t$. Otherwise, our algorithm can produce any label.\n",
    ">\n",
    ">Let $N = |\\mathcal H|$. What is the maximal number of mistakes our implementation of the halving algorithm can make? \n",
    "\n",
    "- if $\\hat{y}_t \\ne y_t$, then $|\\mathcal F_{t+1}| \\le \\frac{1}{\\sqrt{2}}|\\mathcal F_{t}|$\n",
    "- thus after $m$ mistakes, we have $|\\mathcal F_{t}| \\le |\\mathcal H| \\cdot \\frac{1}{\\sqrt{2}^{m}}$\n",
    "- the true hypothesis $f$ always remains in $\\mathcal F_t$ thus \n",
    "\n",
    "$1\\le |\\mathcal F_{t}| \\le |\\mathcal H| \\cdot \\frac{1}{\\sqrt{2}^{m}} \\implies \\frac{1}{N} \\le \\frac{1}{\\sqrt{2}^{m}} \\implies 2^{\\frac{m}{2}} \\le N \\implies \\frac{m}{2} \\le \\log_2 N \\implies m \\le 2\\log_2 N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too many mistakes\n",
    "\n",
    "What if $\\mathcal H=$ set of all binary function on $\\mathcal X$?\n",
    "\n",
    "__Lemma__\n",
    "\n",
    "> For each algorithm $A$ and sequence $x_1, x_2, \\cdots $ there exists an $f$ that makes $|\\{x_1, x_2, \\dots \\}|$ mistakes.\n",
    "\n",
    "(For every algorithm there exists an infinite input sequence where every label is predicted wrongly.)\n",
    "\n",
    "_Proof_. Run the algorithm. For each $x_t$ that was not yet seen, send the opposite label $y_t$ as the predicted $\\hat{y}_t$. $\\mathcal y = \\{0, 1\\}$\n",
    "\n",
    "Receive: $x_1$   \n",
    "Predict: 1 __wrong__  \n",
    "Receive: 0  \n",
    "\n",
    "Receive: $x_2\\ne x_1$   \n",
    "Predict: 0 __wrong__\n",
    "Receive: 1\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "(it means each time we predict a label, we say the correct label is opposite, so on and so on. We get a sequence with all unseen inputs predicted wrongly)\n",
    "\n",
    "When we look at the worst case number of mistakes an algorithm makes, then we see that just reproductive learning is optimal. It's just optimal to remember the inputs that we have already seen and predict anything otherwise. \n",
    "\n",
    "for example, randomly or always the same label, it doesn't matter.\n",
    "\n",
    "### No meaningful mistake bounds\n",
    "\n",
    "Thus for this $\\mathcal H$, reproductive leaning is optimal (just remember things)\n",
    "\n",
    "Principal:\n",
    "\n",
    "_To prove meaningful preformance bounds, we need assumptions on what we \"learn\"_\n",
    "\n",
    "(So we will always prove something, assuming the true functions belong to some class.)\n",
    "\n",
    "Therefore, we consider bounds that only apply to functions in a hypothesis lass $\\mathcal H$\n",
    "\n",
    "We always assume that\n",
    "\n",
    "- $\\mathcal H$ (a set of functions) contains function from $\\mathcal X \\to \\mathcal y$\n",
    "\n",
    "- $\\mathcal X \\to \\mathcal y$ and $\\mathcal H$ are nonempty. (for now we assume $|\\mathcal y| = 2$)\n",
    "\n",
    "__How many mistakes do we learn halfspace in $\\mathcal X =\\mathbb R^p$__\n",
    "\n",
    "(Let us now consider the class $\\mathcal H$ consisting of linear threhold functions. So one label is a halfspace in an $X =\\mathbb R^p$.\n",
    "\n",
    "Is there an algorithm that makes at most poly(p) number of mistakes?\n",
    "\n",
    "No, already learning thresholds in $X=\\mathbb R$ is infeasible in this model. (it's only getting worse as dimension getting higher)\n",
    "\n",
    "Let's prove\n",
    "\n",
    "__Lemma__\n",
    "\n",
    ">For every learning algorithm there esists an $f_r \\in H_{th}$ and infinite sequence $x_1, x_2, x_3, \\dots \\mathcal X$ on which it predicts every label wrong.\n",
    "\n",
    "$y = \\{-1, +1\\}\\;\\; \\mathcal X =\\mathbb R\\;\\; \\mathcal H_{th} = \\{f_r: r \\in \\mathbb R\\}$\n",
    "\n",
    "$f_r(x)=sign(x-r)=\\begin{cases}1 \\;\\;\\; if\\;\\; x \\ge r \\\\ -1 \\;\\;\\;\\;\\; otherwise \\end{cases}$\n",
    "\n",
    "Idea. Move the threshold right or left depending on the predictions of the algorithm. Let $s=\\lim_t x_t$\n",
    "\n",
    "(if a point is predicted to be positive then we move the threshold right and keep the threshold at the right. Otherwise, we moved to the left in the opposite direction, so we are always wrong.)\n",
    "\n",
    "Receive: $x_1=0$ (initial value doesn't matter)   \n",
    "Predict: $\\hat{y}_1=-1$ (the algorithm predicted -1, move the threshold to left)    \n",
    "Receive: $y_1=1$ (inform the algorithm the correct label is 1, so we are wrong)\n",
    "\n",
    "Receive: $x_2=x_1-3^{-1}$   \n",
    "Predict: $\\hat{y}_2=+1$  \n",
    "Predict: $y_2=-1$  \n",
    "\n",
    "(So at this point, we promised to keep the threshold between $x_2$ and $x_1$)\n",
    "\n",
    "Receive: $x_2=x_1-3^{-2}$   \n",
    "Predict: $\\hat{y}_3=+1$  \n",
    "Predict: $y_3=-1$  \n",
    "\n",
    "$ \\large{-} \\;\\;\\;\\;\\;\\;\\;\\; x_2\\bullet \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; x_4\\bullet \\;\\;\\;\\; x_3\\bullet \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; x_1\\bullet \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\large{+} \\mathbb R$\n",
    "\n",
    "_Proof_. Let $x_1=0$, We run A and for $t \\ge 2$, feed it $x_{t+1}=x_t + \\hat{y}_t\\cdot 3^{-t}$\n",
    "\n",
    "Let $s=lim_t x_t = \\sum^{\\infty}_{t=1} \\hat{y}_t 3^{-t}$.  \n",
    "Thus $sign(x_t-s) = sign(\\sum_{j \\ge t}-\\hat{y}_j3^{-j})=sign(-\\hat{y}_t3^{-t}) = -\\hat{y}_t$\n",
    "\n",
    "(We have to look at the sign and for this we know that the first term of the sum determines the sign, which is $y_t$.   \n",
    "There is an extra minus sign before $y_t$, so the true label is always opposite as the predicted label.)\n",
    "\n",
    "(That's how we prove it formally but this lemma is quite obvious)\n",
    "\n",
    "__Conclusions__\n",
    "\n",
    "- We can not obtain meaningful mistake bounds that apply to all binary functions on $\\mathcal X$.  \n",
    "Each result will apply to some subset $\\mathcal H$ of functions of the form $\\mathcal X \\to \\mathcal y$\n",
    "\n",
    "(We can only obtain meaningful mistakes bounds and if we restrict the class of true functions.)\n",
    "\n",
    "- No meaningful bounds for the class of threshold functions in $\\mathbb R$\n",
    "\n",
    "(we proved that the class of threshold functions on R is not learnable and this has two consequences. We cannot learn halfspaces, and we can also not learn decision trees with continuous input in this model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In the movie, we explained that if $\\mathcal H$ is the set of all functions on $\\mathcal X$, then no nontrivial mistake bounds can be proven. Thus if $\\mathcal X$ contains $k$ elements, then for every online prediction algorithm, there exists a true function for which the prediction algorithm  makes at least $k$ errors.\n",
    ">\n",
    ">We consider now the case where we delete a single function, say the constant zero function for convenience, but similar conclusions hold for any other function.\n",
    ">\n",
    ">Let $k \\ge 1$, $\\mathcal X = \\{1,2,\\ldots,k\\}$ and $\\mathcal Y = \\{0,1\\}$. Let $\\mathcal H$ be the set of all functions from $\\mathcal X$ to $\\mathcal Y$ except the constant zero function. \n",
    ">\n",
    ">How many mistakes will the halving algorithm make? Simplify the formula so that no rounding is needed.\n",
    ">\n",
    ">hint:\n",
    ">\n",
    ">If $k = 1$, then $\\mathcal{X} = \\{1\\}$ and $\\mathcal H$ contains only 1 function. \n",
    ">\n",
    ">This is because there are only 2 functions that map elements from $\\mathcal X$ to elements from $\\mathcal Y$: the constant 0 and constant 1 functions, but the first one is not in $\\mathcal H$ by definition of $\\mathcal H$. Hence, the majority vote will always be 1, and the halving algorithm makes 0 mistakes.\n",
    ">\n",
    ">How many functions are there that map $\\mathcal X$ to $\\mathcal Y$? Note that this number is equal to the number of bitstrings of length $k$. Can your answer be equal to $k$? Give an integer lower bound, and note that the difference with the exact value is less than 1.\n",
    "\n",
    "Indeed, the number of functions from $\\mathcal X$ to $\\mathcal Y$ equals $2^k$, thus $|\\mathcal H| = 2^k - 1$. Hence, the binary logarithm is at less than $k$.\n",
    "\n",
    "$log_2(2^k-1) \\approx k-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this problem, the halving algorithm takes a majority vote over $|\\mathcal H| = 2^k-1$ functions.  \n",
    "There exists an easier and much faster algorithm that makes at most $k-1$ mistakes. It can deal with values $k = 1000$ in a fraction of a second. \n",
    "\n",
    "Remeber each correct label, when receiving a input,   \n",
    "if we already have a correct label for it, return the label, give the label  \n",
    "If not, return 1. Because $y \\in \\{0,1\\}$ there is at least 1 input of correct label is 1.  \n",
    "So max mistake is $k-1$\n",
    "\n",
    "Consistent funciton? prediction is consistent with previous seen prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlinePredictor :\n",
    "    def __init__(self, k) :\n",
    "        self.k = k\n",
    "        # if needed, add code here\n",
    "        self.received = None\n",
    "        self.true_f = {}\n",
    "\n",
    "    # if needed, helper functions here\n",
    "\n",
    "    def receive_input_and_predict(self, x):\n",
    "        assert x >= 0 and x < self.k, \"input x out of range\"\n",
    "        # code here, modify the answer below\n",
    "        self.received = x\n",
    "\n",
    "        if x not in self.true_f:\n",
    "            return 1\n",
    "        else:\n",
    "            return self.true_f[x]\n",
    "\n",
    "\n",
    "    def receive_label(self, y) :\n",
    "        # if needed, code here\n",
    "        self.true_f[self.received] = y\n",
    "        # print(self.true_f)\n",
    "\n",
    "    # For local debugging purposes\n",
    "    def count_mistakes(self, input_sequence, true_classifier):\n",
    "        assert len(true_classifier) == self.k and max(true_classifier) == 1 and min(true_classifier) >= 0, \\\n",
    "        \"The true classifier should be a list of integers {0,1} of length k with at least one 1\"\n",
    "        number_mistakes = 0\n",
    "        for x in input_sequence :\n",
    "            y_true = true_classifier[x]\n",
    "            y_predicted = self.receive_input_and_predict(x)\n",
    "            number_mistakes += (y_true != y_predicted)\n",
    "            self.receive_label(y_true)\n",
    "        return number_mistakes\n",
    "\n",
    "# For local debugging\n",
    "op = OnlinePredictor(1)\n",
    "assert op.receive_input_and_predict(0) == 1\n",
    "\n",
    "op = OnlinePredictor(2)\n",
    "assert op.count_mistakes([0, 1, 0, 1, 1], [0, 1]) < 2\n",
    "\n",
    "op = OnlinePredictor(3)\n",
    "assert op.count_mistakes([2, 0, 1, 0], [0, 1, 0]) < 3\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "k = 10\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for _ in range(k):\n",
    "    y.append(random.randint(0, 1))\n",
    "\n",
    "for _ in range(1000):\n",
    "    x.append(random.randint(0, k-1))\n",
    "\n",
    "op = OnlinePredictor(k)\n",
    "mistakes = op.count_mistakes(x, y)\n",
    "assert mistakes < k, mistakes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling methods\n",
    "\n",
    "### Ensembling\n",
    "\n",
    "Base algorithms $b_1, \\dots, b_N$ (some machine learning models)\n",
    "\n",
    "Regression:\n",
    "\n",
    "$a(x)=\\frac{1}{N}\\sum^N_{n=1}b_n(x)$\n",
    "\n",
    "Classification:\n",
    "\n",
    "$a(x)=\\text{argmax}_{y\\in \\mathbb{Y}}\\sum^N_{n=1}[b_n(x) = y] = mode(b_1(x),\\dots,b_N(x))$\n",
    "\n",
    "__Why does ensembling work?__\n",
    "\n",
    "Independent base algorithms $b_1, b_2, b_3$\n",
    "\n",
    "Binary classification task\n",
    "\n",
    "Probability that some $b_i$ makes a mistake: $p$\n",
    "\n",
    "Probability that ensemble makes a mistake: $p^3(3-2p)$\n",
    "\n",
    "if $p \\le 0.5$, then $p^3(3-2p) \\le p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and random subspaces\n",
    "\n",
    "We want to obtain independent base algorithms\n",
    "\n",
    "Train each $b_i$ on some subset of training sample\n",
    "\n",
    "Bagging: use boostrap (taking observations with replacement) to form subsets\n",
    "\n",
    "Random subspaces: use a random subset of features for each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "__Main idea: use ensembling to reduce variance, while bias stays the same__ (smooth border)\n",
    "\n",
    "Base algorithm: decision trees\n",
    "\n",
    "Bagging to make unique train subsets for trees\n",
    "\n",
    "For each split in trees' noeds, use a random subset of features\n",
    "\n",
    "With growing more trees, overfitting is unlikely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "__Main idea: construct ensemble iteratively, correcting the mistakes of previous models__\n",
    "\n",
    "Thus we reduce bias, and by averaging base algorithms we also might reduce variance\n",
    "\n",
    "We can easily overfitting by growing more trees\n",
    "\n",
    "### Boosting: training\n",
    "\n",
    "Boosting model:\n",
    "\n",
    "$a_N(x) = \\sum^N_{n=1}b_n(x)$\n",
    "\n",
    "On the step N, we train a model $b_N$ the following way:\n",
    "\n",
    "$\\sum^l_{i=1}(y_i, a_{N-1}(x_i) + b_N(x_i))\\to\\text{min}_{b_N(x)}$\n",
    "\n",
    "### Boosting: MSE\n",
    "\n",
    "Mean Squared Error loss function:\n",
    "\n",
    "$L(y, \\hat{y}) = (y-\\hat{y})^2$\n",
    "\n",
    "Training boosting with MSE:\n",
    "\n",
    "$\\sum^l_{i=1}(b_N(x_i) - (y_i - a_{N-1}(x_i)))^2 \\to\\text{min}_{b_N(x)}$\n",
    "\n",
    "How to construct $b_N(x)$?\n",
    "\n",
    "Residuals: $s_i^{(N)} = y_i-a_{N-1}(x_i)$\n",
    "\n",
    "Fit the decision tree $b_N$ on residuals and train with MSE:\n",
    "\n",
    "$\\sum^l_{i=1}(b_N(x_i) - s_i^{(N)})^2 \\to\\text{min}_{b_N(x)}$\n",
    "\n",
    "We have some output of our already constructed composition a, with index (N - 1) and we want to know where to move from this answer of composition to approximate this target value. This is why we take residuals as the target value for decision tree, and fit decision tree on them. We can train this residual tree just with the Mean Squared Error.\n",
    "\n",
    "The interesting thing to understand here is that each of this decision tree solves a regression task. It doesn't matter if your initial task was a regression or a classification, still in boosting you construct this boosting ensemble out of regression trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

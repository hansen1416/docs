{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "Helps to combat target leakage problems.\n",
    "\n",
    "__Categorical feature encoding__\n",
    "\n",
    "Extends the idea of target encoding\n",
    "\n",
    "Catboost encoding:\n",
    "\n",
    "- $count(i, j, u_p) = \\sum^l_{k=1}[x_{kj} = u_p, k\\le i]$\n",
    "\n",
    "- $target(i, j, u_p) = \\sum^l_{k=1}[x_{kj} = u_p, k\\le i]y_k$\n",
    "\n",
    "- $\\widetilde{x_{ij}} = \\frac{target(i,j,x_{ij})}{count(i,j,x_{ij})}$\n",
    "\n",
    "- $\\widetilde{x^{\\text{test}}_{ij}} = \\frac{\\sum^l_{k=1}[x_{ij}=x^{\\text{test}}_{ij}]\\widetilde{x_{ij}}}{count(j,x_{ij})}$\n",
    "\n",
    "(For each observation, we take average value of targets. With this category of only previous observations. $k\\le i$. This makes our fitting much smaller. It prevents us from target leakage. It heavily depends on how do we order these observations. You can take some different shuffling and take average)\n",
    "\n",
    "(Same as ordered boosting. In both of these methods, the resulting value for the current observation is computed based only on the previous observations)\n",
    "\n",
    "__Prediction shift__\n",
    "\n",
    "Train a new base algorithm on the step $N$:\n",
    "\n",
    "$s_i^{(N)} = -\\frac{\\partial}{\\partial z} L(y_i, z)|_{z=a_{N-1}}(x_i)$\n",
    "\n",
    "$\\sum^l_{i=1}(b_N(x_i)-s^{(N)}_i)^2 \\to min_{b_N(x)}$\n",
    "\n",
    "$(x_i,y_i)$ is used to train $b_{N-1}(x_i)$ (the previous algorithm), thus a gradient $s^{(N)}_i$ is shifted.\n",
    "\n",
    "It implies that $b_N(x_i)$ is biased, therefore $a_N$ is biased\n",
    "\n",
    "Solution: __Ordered bossting__, at each step, maintain $l$ models where each model $b_{i,N}$ is trained only on the first $i$ shuffled observations\n",
    "\n",
    "$s_i^{(N)} = -\\frac{\\partial}{\\partial z} L(y_i, z)|_{z=b_{i,(N-1)}}(x_i)$\n",
    "\n",
    "(We changed here the formula so that this derivative is taken not in the point of previous composition, but in the corresponding model.)\n",
    "\n",
    "At the last step $T$, we take $b_{l,T}$ to obtain the resulting model. (This is very slow, due to maintain previous models)\n",
    "\n",
    "CatBoost uses an optimized version of this technique\n",
    "\n",
    "__Oblivious trees__\n",
    "\n",
    "In oblivious decision trees, the same splitting criterion is used for the whole tree level.\n",
    "\n",
    "All nodes on the same tree level are split by the same feature.\n",
    "\n",
    "Such trees are balanced, efficient and less prone to overfitting.\n",
    "\n",
    "__Summary__\n",
    "\n",
    "CatBoost proposes several ordering techniques to prevent overfitting\n",
    "\n",
    "For better efficiency, oblivious decisin trees are used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

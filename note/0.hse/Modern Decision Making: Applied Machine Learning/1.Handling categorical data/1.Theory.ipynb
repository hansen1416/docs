{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values\n",
    "\n",
    "Replacing using kNN (k nearest neighbors)\n",
    "- use similiar observations\n",
    "- predict missing values using known ones\n",
    "\n",
    "Pros: May be pretty accurate  \n",
    "Cons: Computational expensive\n",
    "\n",
    "__example__\n",
    "\n",
    ">Suppose that you have an ordinal feature which takes the following values: NaN, 0, 1, 2, 3, 4. Suppose that you want to fill missing values, using a special category. Which value should you use if you don’t want to change feature values distribution?\n",
    "\n",
    "__feature values distribution will change anyway__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic encoding methods\n",
    "\n",
    "### Label encoding\n",
    "\n",
    "- Good for ordinal features.\n",
    "- Bad for nominal features. (Introduced order)\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "- Bad for many nominal features with many categories. (result in too many columns)\n",
    "\n",
    "### Frequency encoding\n",
    "\n",
    "- Bad if frequecies are similar\n",
    "- Helps to merge rare categories into one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding\n",
    "\n",
    "What is the main idea of target encoding? Encode each category with the mean target value of the observations with this category\n",
    "\n",
    "Feature $x_j$ takes values $U_j=\\{u_1, \\dots, u_m\\}$\n",
    "\n",
    "__For each category, estimate:__\n",
    "\n",
    "$u_p \\to s_p \\cong \\mathbb{E}(y | x=u_p)$ (regression)\n",
    "\n",
    "$u_p \\to s_p \\cong \\mathbb{P}(y | x=u_p)$ (classification)\n",
    "\n",
    "__For each category, compute count:__\n",
    "\n",
    "$count(j, u_p) = \\sum^l_{i=1}[x_{ij}=u_p]$\n",
    "\n",
    "j index of attributes, $x_{ij}$ means that this is a value for observation $i$ and for attribute $j$ and this observation is a category $u_p$\n",
    "\n",
    "__For each category, compute target sum:__\n",
    "\n",
    "- regression:\n",
    "\n",
    "$target(j, u_p) = \\sum^{l}_{i=1}[x_{ij}=u_p]y_i$\n",
    "\n",
    "- classification:\n",
    "\n",
    "$target_k(j, u_p) = \\sum^{l}_{i=1}[x_{ij}=u_p][y_i=k]$\n",
    "\n",
    "__Encode__\n",
    "\n",
    "- regression:\n",
    "\n",
    "$\\widetilde{x_{ij}} = \\frac{target(j, x_{ij})}{count(j,x_{ij})}$\n",
    "\n",
    "- classification:\n",
    "\n",
    "$\\widetilde{x_{ij}} = (\\frac{target_1(j, x_{ij})}{count(j,x_{ij})}, \\dots, \\frac{target_K(j, x_{ij})}{count(j,x_{ij})} )$\n",
    "\n",
    "- Store information about target variable. So it may improve your model performance significantly. \n",
    "- __Leads to overfitting (target leakage)__\n",
    "\n",
    "(basically, we calculated features from target)\n",
    "\n",
    "__example__\n",
    "\n",
    ">Suppose that you are solving a regression task and you want to encode one categorical feature. Five observations in the data correspond to the category A and have the following target values: 2, 5, 6, 10, 12. What will be the number to encode the category A if you perform target encoding?\n",
    "\n",
    "sum / count = 35/5 = 7\n",
    "\n",
    "> Suppose that you have an ID feature, where each category appears only one time. Is it a good idea to use target encoding to encode it?\n",
    "\n",
    "No - it will lead to target leakage. Due to the structure of this feature, you will simply get a target column as a new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding: modifications\n",
    "\n",
    "__How to prevent overfitting?__\n",
    "\n",
    "__Solution 1: add noise__\n",
    "\n",
    "$\\widetilde{x_{ij}} = \\widetilde{x_{ij}} + \\varepsilon$\n",
    "\n",
    "- \"Hides\" the target signal by the encoding degradation\n",
    "- Need to tune the amount of noise\n",
    "\n",
    "__Solution 2: smoothing__\n",
    "\n",
    "Use weighted sum of target encoding and target average values\n",
    "\n",
    "$\\widehat{x_{ij}} = \\lambda(n) * \\widetilde{x_{ij}} + (1 - \\lambda(n)) * y_{mean}$\n",
    "\n",
    "- $n = count(j, x_{ij})$\n",
    "- $\\lambda(n)$ - monotonically increasing function, bounded between 0 and 1\n",
    "- e.g. $\\lambda(n) = \\frac{1}{1+exp(-\\frac{n-k}{f})}$\n",
    "\n",
    "$n \\to \\infty, \\lambda(n) = 1: x_{ij} = \\widetilde{x_{ij}}$\n",
    "\n",
    "$n \\to 0, \\lambda(n) = 0: x_{ij} = y_{mean}$\n",
    "\n",
    "If the category is frequent, in the categorical attributes, then we use regular target encoding without any other features.  \n",
    "However, if the category is rare, that is the count, the frequency of this category is going to zero. this Lambda is going to zero. We have only an average of targets for the whole data sets.  \n",
    "Because, for rare categories we suffer most from overfitting.\n",
    "\n",
    "__Solution 2: cross validation__\n",
    "\n",
    "Compute $\\widetilde{x_{ij}}$ based on the target of other fold. We obtain k different encoders for k folds.\n",
    "\n",
    "For each encoded parts of test data, you compute model output like probabilities and classification and then by using the average of these outputs, and then you obtain final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced encoding methods: M-estimate, Leave-One-Out\n",
    "\n",
    "__M-estimate encoding:__\n",
    "\n",
    "Use a specific function for smoothing target encoding\n",
    "\n",
    "$\\lambda(n) = \\frac{n}{m+n}$\n",
    "\n",
    "$\\widehat{x_{ij}} = \\frac{target(j, x_{ij}) + m * y_{mean}}{count(j, x_{ij}) + m}$\n",
    "\n",
    "__Leave-one-out encoding:__\n",
    "\n",
    "Here we compute the frequency of the category, not on the whole dataset, but excluding this one observation and the target is the same.\n",
    "\n",
    "How do we represent this category if it appears in some tests observation? We average all encoding values for this single category to compute its representation for a test observation.\n",
    "\n",
    "$count(i, j, u_p) = \\sum^l_{k=1}[x_{kj} = u_p, k\\ne i]$\n",
    "\n",
    "$target(i, j, u_p) = \\sum^l_{k=1}[x_{kj} = u_p, k\\ne i]y_k$\n",
    "\n",
    "$\\widetilde{x_{ij}} = \\frac{target(i,j,x_{ij})}{count(i,j,x_{ij})}$\n",
    "\n",
    "$\\widetilde{x_{ij}^{test}} = \\frac{\\sum^l_{k=1}[x_{ij} = x_{ij}^{test}] \\widetilde{x_{ij}}}{count(i, x_{ij})}$\n",
    "\n",
    "- Helps to deal with outliers\n",
    "- Induces shift for rare categories\n",
    "- __Actually it does not prevent target leakage__\n",
    "\n",
    "__Leave-one-out encoding: overfitting__\n",
    "\n",
    "$target(j, u_p) = \\sum^l_{i=1}[x_{ij} = u_p]y_i = target(i,j,u_p) + y_i$\n",
    "\n",
    "Specify $t=\\frac{target(j, x_{ij}) - 0.5}{count(i,j,x_{ij})}$ (threshold)\n",
    "\n",
    "- $y_k \\ge 0.5 \\iff \\widetilde{x_{ij}} \\le t$\n",
    "- $y_k < 0.5 \\iff \\widetilde{x_{ij}} > t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced encoding methods: Catboost, WoE\n",
    "\n",
    "__Catboost encoding:__\n",
    "\n",
    "$count(i, j, u_p) = \\sum^l_{k=1}[x_{kj} = u_p, k \\le i]$\n",
    "\n",
    "$target(i, j, u_p) = \\sum^l_{k=1}[x_{kj} = u_p, k \\le i]y_k$\n",
    "\n",
    "$\\widetilde{x_{ij}} = \\frac{target(i,j,x_{ij})}{count(i,j,x_{ij})}$\n",
    "\n",
    "$\\widetilde{x_{ij}^{test}} = \\frac{\\sum^l_{k=1}[x_{ij} = x_{ij}^{test}] \\widetilde{x_{ij}}}{count(i, x_{ij})}$\n",
    "\n",
    "Requires shuffling data several times (probably take average of all calculated categories, prevent some target leakage)\n",
    "\n",
    "__WoE encoding__\n",
    "\n",
    "Weight of Evidence encoding: (for binary classification)\n",
    "- Binary classification: bad(0) vs good(1)\n",
    "- $\\mathbb{P}(x_{ij} | y=1) = \\frac{count(y=1 | x_{ij})}{count(y=1)}$\n",
    "- $\\mathbb{P}(x_{ij} | y=0) = \\frac{count(y=0 | x_{ij})}{count(y=0)}$\n",
    "- $\\widetilde{x_{ij}} = \\ln(\\frac{\\mathbb{P}(x_{ij} | y=1)}{\\mathbb{P}(x_{ij} | y=0)}) * 100$\n",
    "\n",
    "The idea is:\n",
    "\n",
    "- $\\widetilde{x_{ij}} = 0 \\iff \\mathbb{P}(x_{ij} | y=1) = \\mathbb{P}(x_{ij} | y=0)$: random (the category doesn't have much value)\n",
    "- $\\widetilde{x_{ij}} > 0 \\iff \\mathbb{P}(x_{ij} | y=1) > \\mathbb{P}(x_{ij} | y=0)$: goods\n",
    "- $\\widetilde{x_{ij}} < 0 \\iff \\mathbb{P}(x_{ij} | y=1) < \\mathbb{P}(x_{ij} | y=0)$: bads\n",
    "\n",
    "Smoothing：\n",
    "\n",
    "$\\mathbb{P}(x_{ij} | y=1) = \\frac{count(y=1 | x_{ij}) + \\alpha}{count(y=1) + 2\\alpha}$\n",
    "\n",
    "$\\mathbb{P}(x_{ij} | y=0) = \\frac{count(y=0 | x_{ij}) + \\alpha}{count(y=0) + 2\\alpha}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

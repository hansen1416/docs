{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General formulation: Classfification\n",
    "\n",
    "- Let us train $M$ models: $b_1(x), \\dots, b_M(x)$ (train M models separately)\n",
    "- composition through majority vote:\n",
    "\n",
    "$a_M(x) = argmax_{y \\in \\mathbb{Y}} \\sum^{M}_{m=1}[b_m(x)=y]$ (predict separately, choose the most popular result among all models)\n",
    "\n",
    "## General formulation: Regression\n",
    "\n",
    "- Let us train $M$ models: $b_1(x), \\dots, b_M(x)$ (train M models separately)\n",
    "- composition through averaging:\n",
    "\n",
    "$a_M(x) = \\frac{1}{M} \\sum^{M}_{m=1}b_m(x)$\n",
    "\n",
    "How to build M models, if we have only 1 training dataset? (there is no use if all models are the same)\n",
    "\n",
    "Two options:\n",
    "\n",
    "- Train independently on different subsamples\n",
    "    - Bagging\n",
    "    - Random subsapces\n",
    "    \n",
    "- Train successively to correct error of the previous algorithm\n",
    "    - Boosting\n",
    "    \n",
    "    \n",
    "__Bagging__\n",
    "\n",
    "- Bagging (bootstrap aggregating).\n",
    "- Base algorithms are trained independently.\n",
    "- Eahc model is trained on the subsample of the training dataset.\n",
    "- The subsample is obtained by bootstrap.\n",
    "\n",
    "__Bootstrap__\n",
    "\n",
    "- Sample with replacement\n",
    "- Select N element from X (where N is the size of the trainning dataset). (equal chance of getting a certain item each time. so there could be repetition)\n",
    "\n",
    "    - e.g. $\\{x_1, x_2, x_3, x_4\\} \\to \\{x_1, x_2, x_2, x_4\\}$\n",
    "    \n",
    "- Our subsample will have approximately 63.2% unique observations\n",
    "\n",
    "__Random subsapces__\n",
    "\n",
    "- Base algorithm are trained independently\n",
    "- Use random subsets of features for each model\n",
    "- May work poorly if some features is crucial for a reasonable model\n",
    "\n",
    "__Source of randomness__\n",
    "\n",
    "- Bagging: random subset (random rows)\n",
    "- Random subsapces: random subset of features. (random columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "__Bias-variance trade-off__\n",
    "\n",
    "$\\mathbb{E}[y-a(x)]^2 = bias^2 + Variance$\n",
    "\n",
    "Model is more complex, bias is lower, Variance is higher, and vice versa.\n",
    "\n",
    "__Linear models__: High bias and low Variance. (Because it id very simple, insensitive to data)\n",
    "\n",
    "__Decision trees__: Low bias and high Variance. (Sensitive to data, easily overfit)\n",
    "\n",
    "### Bias and variance for composition\n",
    "\n",
    "__Linear models__\n",
    "\n",
    "- Bias: $a_M(x)$ has the same bias as $b_m(x)$\n",
    "\n",
    "- Variance: $Var(a_M(x))=\\frac{1}{M}(Var\\; b_m(x)) + Cov(b_m(x), b_k(x))$\n",
    "\n",
    "- If base models are independent, variance is M times smaller\n",
    "- The more correlated models are, the less is effect of the ensemble. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

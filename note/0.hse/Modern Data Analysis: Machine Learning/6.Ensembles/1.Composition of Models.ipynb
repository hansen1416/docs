{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General formulation: Classfification\n",
    "\n",
    "- Let us train $M$ models: $b_1(x), \\dots, b_M(x)$ (train M models separately)\n",
    "- composition through majority vote:\n",
    "\n",
    "$a_M(x) = argmax_{y \\in \\mathbb{Y}} \\sum^{M}_{m=1}[b_m(x)=y]$ (predict separately, choose the most popular result among all models)\n",
    "\n",
    "## General formulation: Regression\n",
    "\n",
    "- Let us train $M$ models: $b_1(x), \\dots, b_M(x)$ (train M models separately)\n",
    "- composition through averaging:\n",
    "\n",
    "$a_M(x) = \\frac{1}{M} \\sum^{M}_{m=1}b_m(x)$\n",
    "\n",
    "How to build M models, if we have only 1 training dataset? (there is no use if all models are the same)\n",
    "\n",
    "Two options:\n",
    "\n",
    "- Train independently on different subsamples\n",
    "    - Bagging\n",
    "    - Random subsapces\n",
    "    \n",
    "- Train successively to correct error of the previous algorithm\n",
    "    - Boosting\n",
    "    \n",
    "    \n",
    "__Bagging__\n",
    "\n",
    "- Bagging (bootstrap aggregating).\n",
    "- Base algorithms are trained independently.\n",
    "- Eahc model is trained on the subsample of the training dataset.\n",
    "- The subsample is obtained by bootstrap.\n",
    "\n",
    "__Bootstrap__\n",
    "\n",
    "- Sample with replacement\n",
    "- Select N element from X (where N is the size of the trainning dataset). (equal chance of getting a certain item each time. so there could be repetition)\n",
    "\n",
    "    - e.g. $\\{x_1, x_2, x_3, x_4\\} \\to \\{x_1, x_2, x_2, x_4\\}$\n",
    "    \n",
    "- Our subsample will have approximately 63.2% unique observations\n",
    "\n",
    "__Random subsapces__\n",
    "\n",
    "- Base algorithm are trained independently\n",
    "- Use random subsets of features for each model\n",
    "- May work poorly if some features is crucial for a reasonable model\n",
    "\n",
    "__Source of randomness__\n",
    "\n",
    "- Bagging: random subset (random rows)\n",
    "- Random subsapces: random subset of features. (random columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "__Bias-variance trade-off__\n",
    "\n",
    "$\\mathbb{E}[y-a(x)]^2 = bias^2 + Variance$\n",
    "\n",
    "Model is more complex, bias is lower, Variance is higher, and vice versa.\n",
    "\n",
    "__Linear models__: High bias and low Variance. (Because it is very simple, insensitive to data)\n",
    "\n",
    "__Decision trees__: Low bias and high Variance. (Sensitive to data, easily overfit)\n",
    "\n",
    "### Bias and variance for composition\n",
    "\n",
    "__Linear models__\n",
    "\n",
    "- Bias: $a_M(x)$ has the same bias as $b_m(x)$\n",
    "\n",
    "- Variance: $Var(a_M(x))=\\frac{1}{M}(Var\\; b_m(x)) + Cov(b_m(x), b_k(x))$\n",
    "\n",
    "- If base models are independent, variance is M times smaller\n",
    "- The more correlated models are, the less is effect of the ensemble. \n",
    "\n",
    "__Decision tree: Greedy Algorithm__\n",
    "\n",
    "1. Put the whole dataset into the root: $R_1= X$\n",
    "\n",
    "2. Start the tree construction: SplitNode$(1,R_1)$\n",
    "\n",
    "SplitNode$(1,R_1)$:\n",
    "\n",
    "1. If stopping criterion is met, then quit\n",
    "\n",
    "2. Find the best split (feature and threshold): $j,t=argmax_{j,t}Q(R_m,j,t)$\n",
    "\n",
    "3. Split objects: $R_l = \\{ (x,y) \\in R_m | [x_j < t] \\}, R_r = \\{ (x,y) \\in R_m | [x_j \\ge t]\\} $\n",
    "\n",
    "4. Repeat for the child nodes: SplitNode$(l, R_l)$ and SplitNode$(r, R_r)$\n",
    "\n",
    "__We modify $j,t=argmax_{j,t}Q(R_m,j,t)$, not select all features, instead we select a random subset of features of size q. Select randomly for each model__\n",
    "\n",
    "Turns out that if number of features that you randomly select for each split is small, then the correlation of the resulting trees will also be very small. (Low variance for composition)\n",
    "But small size of features lead to larger bias.\n",
    "\n",
    "Recommended values of q:\n",
    "- Regression task: $q=\\frac{d}{3}$\n",
    "- Classification task: $q=\\sqrt{d}$\n",
    "where d is the total number of features in the dataset. (just approximation, better tune it)\n",
    "\n",
    "### Random Forest algorithm\n",
    "\n",
    "1. For $m=1,2,3,\\dots,M$:\n",
    "\n",
    "2. Sample $\\tilde{X}$ from the training dataset using bootstrap. (bagging)\n",
    "\n",
    "3. Train decision tree $b_m(x)$ on $\\tilde{X}$\n",
    "    - Stopping criterion: $n_{min}$ objects in the leaf\n",
    "    - __Select q feature randomly before selecting optimal split on each node__.\n",
    "    \n",
    "__Random Forest__\n",
    "\n",
    "Regression: $a_M(x) = \\frac{1}{M} \\sum^{M}_{m=1}b_m(x)$\n",
    "\n",
    "Classfification: $a_M(x) = argmax_{y \\in \\mathbb{Y}} \\sum^{M}_{m=1}[b_m(x)=y]$\n",
    "\n",
    "_It doesn't overfit even for large M, becasue each tree is trained independently._\n",
    "\n",
    "__out-of-bag__\n",
    "\n",
    "- Each tree uses approximately 63% of the observations\n",
    "- The rest can be used as a validation set\n",
    "- $X_m$ - training dataset for $b_m(x)$\n",
    "- We can estimate error:\n",
    "\n",
    "$L_{val} = \\frac{1}{N}\\sum^{N}_{i=1} l(y_i, \\frac{1}{\\sum^{M}_{m=1}[x_i \\notin X_m]} \\sum^{M}_{m=1}[x_i \\notin X_m] b_m(x_i))$\n",
    "\n",
    "_We can evaluate generaliztion ability without validation set._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

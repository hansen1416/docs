{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting idea\n",
    "\n",
    "- Let us use very simple base models (weak learners)\n",
    "- We will construct an ensemble successively and greedily\n",
    "- Each successive model will try to correct the errors of the previous ones\n",
    "\n",
    "$a_M(x) = \\sum^{M}_{m=1}b_m(x)$\n",
    "\n",
    "Training the __first__ base model:\n",
    "\n",
    "$min_{b_1(x)}L(b_1, X) = min_{b_1(x)}\\frac{1}{N} \\sum^{N}_{i=1}l(y_i, b_1(x_i))$\n",
    "\n",
    "Training the __M__th base model:\n",
    "\n",
    "$min_{b_M(x)}\\frac{1}{N} \\sum^{N}_{i=1}l(y_i, a_{M-1}(x_i)+b_1(x_i))$\n",
    "\n",
    "The main question is how to learn a tree with such loss in general case. (Very complex loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting for MSE Loss\n",
    "\n",
    "Training the __M__th base model:\n",
    "\n",
    "$min_{b_M(x)}\\frac{1}{N} \\sum^{N}_{i=1}(b_M(x_i) - (a_{M-1}(x_i) - y_i))^2$\n",
    "\n",
    "Residuals - $s^{(M)}_i = a_{M-1}(x_i) - y_i$\n",
    "\n",
    "__First iteration__\n",
    "\n",
    "$\\frac{1}{N}\\sum^{N}_{i=1}(b_1(x_i)-y_i)^2 \\to min_{b_1(x)}$\n",
    "\n",
    "__Second iteration__\n",
    "\n",
    "$\\frac{1}{N}\\sum^{N}_{i=1}(b_2(x_i)-(y_i - b_1(x_i)))^2 \\to min_{b_2(x)}$ (minimize the loss with respect to $b_2$)\n",
    "\n",
    "__Third iteration__\n",
    "\n",
    "$\\frac{1}{N}\\sum^{N}_{i=1}(b_3(x_i)-(y_i - b_1(x_i) - b_2(x_i)))^2 \\to min_{b_3(x)}$\n",
    "\n",
    "- Boosting is prone to overfitting, thus it is essential to use validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulties with the General Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting: General Form\n",
    "\n",
    "__Training a base model__\n",
    "\n",
    "$M$th base model:\n",
    "\n",
    "$\\frac{1}{N}\\sum^{N}{i=1}l(y_i, a_{M-1}(x_i)+b_M(x_i)) \\to min_{b_M(x)}$\n",
    "\n",
    "How to decide, in which direction and how hard to change $a_{M-1}(x_i)$ in order to reduce error.\n",
    "\n",
    "- Calculate the gradient\n",
    "\n",
    "$s^{(M)}_i = -\\frac{\\partial}{\\partial z}l(y_i, z)|_{z=a_{M-1}(x_i)}$ - residuals\n",
    "\n",
    "- The sign shows the direction in whcih we should change the prediction for the object $x_i$, to reduce the error of the composition on it.\n",
    "\n",
    "- The absolute value shows how large the decrease in the error will be.\n",
    "\n",
    "- If there is no decrease in the error, there is no point in changing the prediction.\n",
    "\n",
    "Training $M$th base model:\n",
    "\n",
    "$\\frac{1}{N}\\sum^{N}_{i=1}(b_M(x_i)-s_i^{(M)})^2 \\to min_{b_M(x)}$\n",
    "\n",
    "To sum up, we will calculate the residuals of the current composition for each training point as a gradient. Then we will train and the next base model to feed these residuals. In this case, we can always use mean squared error loss, because the specificity of the loss function was already included in the calculation of residuals. \n",
    "\n",
    "We can think of this idea as if we're doing gradient descent in the space of answers on the training data-set. \n",
    "\n",
    "Each new base model will correct the prediction of the algorithm so that the total error of our composition will become smaller in terms of the loss function that we want.\n",
    "\n",
    "Residuals take into account specific loss function.\n",
    "\n",
    "__Gradient boosting for MSE__\n",
    "\n",
    "$s^{(M)}_i=-\\frac{\\partial}{\\partial z}l(y_i,z)|_{z=a_{M-1}(x_i)} = -\\frac{\\partial}{\\partial z}\\frac{1}{2}(z-y_i)^2|_{z=a_{M-1}(x_i)} = -(a_{M-1}(x_i) - y_i) = y_i - a_{M-1}(x_i)$\n",
    "\n",
    "$\\frac{1}{N}\\sum^{N}_{i=1}(b_M(x_i)-(y_i - a_{M-1}(x_i)))^2 \\to min_{b_M(x)}$ (same as discussed before)\n",
    "\n",
    "__Gradient boosting for logistic loss__\n",
    "\n",
    "$s^{(M)}_i=-\\frac{\\partial}{\\partial z}l(y_i,z)|_{z=a_{M-1}(x_i)} = -\\frac{\\partial}{\\partial z} \\log(1+\\exp(-y_iz))|_{z=a_{M-1}(x_i)} = \\frac{y_i}{1+\\exp(y_i a_{M-1}(x_i))}$ (logistic loss)\n",
    "\n",
    "$\\frac{1}{N}\\sum^{N}_{i=1}(b_M(x_i) - \\frac{y_i}{1+\\exp(y_i a_{M-1}(x_i))} )^2 \\to min_{b_M(x)}$ (after calculated residuals, apply MSE, to train the new base model to fit this residuals)\n",
    "\n",
    "- Large positive margin: $\\frac{y_i}{1+\\exp(y_i a_{M-1}(x_i))} \\approx 0$  \n",
    "($y_i$ and $a_{M-1}(x_i)$ has same sign, denominator is large. which means the previous prediction is accurate, we don't want to change it much)\n",
    "\n",
    "- Large negative margin: $\\frac{y_i}{1+\\exp(y_i a_{M-1}(x_i))} \\approx \\pm1$  \n",
    "(we are confident we are wrong, we will want to change the total composition by training our base classifier to move either to the positive or to the negative side. )\n",
    "\n",
    "When we use gradient and boosting, we incorporate properties of a specific loss-function by calculating the gradient or fit for all training points. \n",
    "\n",
    "Then we will try to feed our base model so that it improves the total composition in terms of this loss. We can then train the base model using mean squared error with the residuals as a target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Hyperparameters\n",
    "\n",
    "Tree depth\n",
    "\n",
    "- Gradient boosting reduces the bias of base models\n",
    "- Variance may increase\n",
    "- Hence, it is worth using simple algorithms. e.g.trees with small depth (make it simple)\n",
    "    - Fix the depth\n",
    "    - Fix maximal number of leaves\n",
    "    \n",
    "- Residuals show the desired direction on the whole training dataset\n",
    "\n",
    "- Base models are supposed to be simple and may not be powerful enough to fit the residuals.\n",
    "\n",
    "    __Solution__: add new trees to the composition with small weight. In gradient descent, we're having learning rate which controls the size of the steps that we're making. The same happens here, when we add the new base model to a composition, we can use a step size $\\mu$, which is usually less than one, in order to somehow regularize our composition.\n",
    "    \n",
    "    __Step size__\n",
    "    \n",
    "    $a_M(x)=a_{M-1}(x_i) + \\mu b_M(x_i)$\n",
    "    \n",
    "    - $\\mu \\in (0,1]$ step size\n",
    "    - One can think it is a regularization of the composition\n",
    "    - We reduce the influence of each model\n",
    "    - The smaller the step size is, the more trees are needed\n",
    "    \n",
    "    If the step size is very large, we firstly converge to a very good solution. But then if we keep adding trees,  the error on the test set may start increasing. On the same time, if the step size is small, it takes more time to achieve good performance, but at the end, we might even end up in a better solution.\n",
    "    \n",
    "    __Feature randomization__ (another option of regularize our Gradient Boosting)\n",
    "    \n",
    "    - We can train trees on random subspaces\n",
    "    - Boosting reduces bias, therefore the final composition will be still good enough\n",
    "    - At the same time, it may reduce overfitting.\n",
    "    \n",
    "### Hyperparameters\n",
    "- Tree depth\n",
    "- Total number of trees\n",
    "- Step size\n",
    "- Size of the subset (used to train a tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
